<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>hive configuration understanding</title>
      <link href="/2018/11/19/hive-configuration-best-practise/"/>
      <url>/2018/11/19/hive-configuration-best-practise/</url>
      
        <content type="html"><![CDATA[<p>This article aims at introducing what are the manually configured settings that override the default during using hive.</p><h2 id="environment"><a href="#environment" class="headerlink" title="environment"></a>environment</h2><p>This article is based on hive 2.3, hadoop 2.7, running hive on mapreduce.</p><h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>Hive and hadoop have many configurations and it is not striaightforward to know what configurations need to be override manually. This article aims at listing up all such configurations that we can special take care of.</p><h2 id="configuration-reference"><a href="#configuration-reference" class="headerlink" title="configuration reference"></a>configuration reference</h2><p>There are the links of the latest explanations of the configurations in the official wiki . </p><ul><li>hadoop<ul><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">core-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">hdfs-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="noopener">mapred-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a></li></ul></li><li>hive<ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties" target="_blank" rel="noopener">hive-site.xml</a></li></ul></li></ul><h2 id="overridden-configurations"><a href="#overridden-configurations" class="headerlink" title="overridden configurations"></a>overridden configurations</h2><p>These are the configurations that need to take special care and be configured manually to override the default value.</p><ul><li><p>Basic settings</p><ul><li>purpose<ul><li>There are some basic settings.</li></ul></li><li>configuration<ul><li>fs.defaultFS<ul><li>default : file:///</li><li>configured :  hdfs://&lt;namenode_ip&gt;:8020</li><li>explain : This configures the default file system. By default, it is local file system, change it to use distributed file system.</li></ul></li><li>mapreduce.framework.name<ul><li>default : local</li><li>configured : yarn</li><li>explain : The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.</li></ul></li><li>yarn.resourcemanager.hostname <ul><li>default:  0.0.0.0</li><li>configured: <the real="" ip=""></the></li><li>explain: The hostname of the RM.</li></ul></li></ul></li></ul></li><li><p>Enable yarn log aggregation</p><ul><li>purpose <ul><li>By default, yanr log aggregation is disabled, and yarn logs(container logs) are saved in the local of the instances where the containers run. In order to check the log, it is needed to first check the instance ip of a container, then login the instance and open the corresponding log file. When yarn log aggregation is enabled, the container’s log will be aggregated to some configured place(HDFS, s3, etc.), and it is able to check the yarn log through the yarn UI, which becomes much simpler.</li></ul></li><li>configuration <ul><li>yarn.log-aggregation-enable<ul><li>default : false</li><li>configured : true</li><li>explain : This enables the yarn log aggregation.</li></ul></li><li>yarn.nodemanager.remote-app-log-dir<ul><li>default : /tmp/logs for hdfs</li><li>configured : e.g. s3a://… for s3</li><li>explain : This specifies the prefix of the actual path of the aggregated logs. You may want to include the cluster name in the path.</li></ul></li><li>yarn.log-aggregation.retain-seconds<ul><li>default : -1</li><li>explain : This specifies how long the aggregated logs will be kept. The default value -1 indicates keeping the logs forever. Too big value may cause a waste of disk resource, too small may make you cannot find logs when investigate recent issues. 2 weeks or so may be helpful.</li></ul></li><li>yarn.nodemanager.log-aggregation.compression-type<ul><li>default: none</li><li>configured: gz</li><li>explain: If, and how, aggregated logs should be compressed.</li></ul></li><li>mapreduce.job.userlog.retain.hours<ul><li>default : 24</li><li>explain : This specifies the maximum time in hours that the local yarn logs will be retained after job completion. You may want to adjust this setting together.</li></ul></li><li>mapreduce.jobhistory.max-age-ms<ul><li>default : 604800000</li><li>explain : Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week). We can make it 2 weeks.</li></ul></li></ul></li><li><a href="https://renenyffenegger.ch/notes/development/Apache/Hadoop/Ycccc-=ARN/log-aggregation" target="_blank" rel="noopener">helpful links</a></li></ul></li><li>Clean HDFS trash regularly<ul><li>purpose<ul><li>By default, the trash folder is not cleaned up automatically. We can configure to make it be cleaned up regularly to reduce disk cost.</li></ul></li><li>configuration<ul><li>fs.trash.interval    <ul><li>default : 0</li><li>configured : 360</li><li>explain : will be deleted after 6 hours</li></ul></li></ul></li></ul></li><li>Enable hive client authorization<ul><li>purpose<ul><li>By default, hive client authorization is disabled. Enabling hive client authorization can help prevent users from doing operations they are not supposed to do.</li></ul></li><li>configuration<ul><li>hive.security.authorization.enabled<ul><li>default : false</li><li>configured : true</li><li>explain : Enables the hive client authorization.</li></ul></li><li>There are also other settings as needed. I will leave it as TODO.</li></ul></li><li>helpful links<ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization" target="_blank" rel="noopener">SQL Standard Based Hive Authorization</a></li></ul></li></ul></li><li>Enable HDFS compression<ul><li>configuration<ul><li>io.compression.codecs<ul><li>default : </li><li>configured :  org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec</li><li>explain : A comma-separated list of the compression codec classes that can be used for compression/decompression which precedence of others loaded by from the classpath.</li></ul></li><li>mapreduce.map.output.compress<ul><li>default : false</li><li>configured : true</li><li>explain : Compress the output of map before sending accross the network.</li></ul></li><li>mapreduce.map.output.compress.codec<ul><li>default : org.apache.hadoop.io.compress.DefaultCodec</li><li>configured : org.apache.hadoop.io.compress.SnappyCodec or org.apache.hadoop.io.compress.GzipCodec</li><li>explain : If the map outputs are compressed, how should they be compressed?</li></ul></li><li>mapreduce.output.fileoutputformat.compress.type<ul><li>default : RECORD</li><li>configured : BLOCK</li><li>explain : If the job outputs are to compressed as SequenceFiles, how should they be compressed?</li></ul></li></ul></li></ul></li><li>Fail jobs when exceed disk usage<ul><li>configuration<ul><li>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<ul><li>default : true</li><li>configured : false</li></ul></li></ul></li></ul></li><li>dfs related<ul><li>configuration<ul><li>dfs.namenode.avoid.read.stale.datanode<ul><li>default : false</li><li>configured : true</li></ul></li><li>dfs.namenode.avoid.write.stale.datanode<ul><li>default : false</li><li>configured : true</li></ul></li></ul></li></ul></li><li>save job result in s3(option)<ul><li>configuration<ul><li>mapreduce.jobhistory.done-dir <ul><li>default: ${yarn.app.mapreduce.am.staging-dir}/history/done </li><li>configured: s3 path</li></ul></li></ul></li></ul></li><li>speculative configs<ul><li>configuration<ul><li>mapreduce.map.speculative <ul><li>default: true</li><li>configured: false</li></ul></li><li>mapreduce.reduce.speculative<ul><li>default: true</li><li>configured: false</li></ul></li></ul></li></ul></li><li>enable namenode restart<ul><li>configuration<ul><li>yarn.nodemanager.recovery.enabled<ul><li>default: false</li><li>configured: true</li></ul></li><li>yarn.nodemanager.address<ul><li>default:  ${yarn.nodemanager.hostname}:0</li><li>configured: 0.0.0.0:45454</li><li>explain: Default value makes namenode use different port before and after a restart, and the running clients will be broken after the restart, explicitly setting the port can solve the issue.</li></ul></li></ul></li><li><a href="https://hadoop.apache.org/docs/r2.8.0/hadoop-yarn/hadoop-yarn-site/NodeManager.html" target="_blank" rel="noopener">helpful links</a></li></ul></li><li>enable resource manager restart<ul><li>configuration<ul><li>yarn.resourcemanager.recovery.enabled<ul><li>default: false</li><li>configured: true</li></ul></li></ul></li><li><a href="https://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">helpful link</a></li></ul></li><li>hive configs<ul><li>configuration<ul><li>hive.auto.convert.join<ul><li>configured: false</li></ul></li><li>hive.auto.convert.join.noconditionaltask<ul><li>configured: false</li></ul></li><li>hive.exec.compress.intermediate<ul><li>configured: true</li></ul></li><li>hive.exec.parallel<ul><li>configured: true</li></ul></li><li>hive.fetch.task.conversion<ul><li>default: more</li><li>configured: none</li></ul></li><li>hive.groupby.orderby.position.alias<ul><li>configured: true</li></ul></li><li>hive.log.explain.output<ul><li>configured: true</li></ul></li><li>hive.mapred.reduce.tasks.speculative.execution<ul><li>configured: false</li></ul></li><li>hive.optimize.reducededuplication<ul><li>configured: false</li></ul></li><li>hive.resultset.use.unique.column.names<ul><li>configured: false</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive scratch directory</title>
      <link href="/2018/11/17/hive-scratch-working-directory/"/>
      <url>/2018/11/17/hive-scratch-working-directory/</url>
      
        <content type="html"><![CDATA[<p>This article aims at explaining hive scratch directory.</p><h2 id="Scratch-directory-usage"><a href="#Scratch-directory-usage" class="headerlink" title="Scratch directory usage"></a>Scratch directory usage</h2><p>Hive scratch directory is a temporary working space for storing the plans for different map/reduce stages of the query as well as the intermediate outputs of these stages.</p><h2 id="Scratch-directory-clean-up"><a href="#Scratch-directory-clean-up" class="headerlink" title="Scratch directory clean up"></a>Scratch directory clean up</h2><p>Hive scratch directory is usually cleaned up by the hive client when the query finishes. However, some data may be left behind if hive client terminates abnormally. Hive server2 contains a thread (<a href="https://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/session/ClearDanglingScratchDir.java#L43-L55" target="_blank" rel="noopener">ClearDanglingScratchDir</a>) to clean up the remaining files, we can also write our own script to do the clean up if not running Hive server2.</p><h2 id="Scratch-directory-types"><a href="#Scratch-directory-types" class="headerlink" title="Scratch directory types"></a>Scratch directory types</h2><p>Hive queries may be procesed in local(the instance which hive client is invoked) or in remote(hadoop cluster). There also have two kinds of scratch dir accordingly, one in local, the other in hdfs.</p><h2 id="Scratch-directory-configuration"><a href="#Scratch-directory-configuration" class="headerlink" title="Scratch directory configuration"></a>Scratch directory configuration</h2><p><em>hive.exec.local.scratchdir</em> for local and <em>hive.exec.scratchdir</em> for HDFS(<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">hive configuration</a>).</p><p>Note that since hive 0.14.0, the HDFS scratch directory created will be <em>${hive.exec.scratchdir}\${user_name}</em> indicating it supports multi-tenant natively and there is no need to include user_id in the value.</p><h2 id="Scratch-directory-example"><a href="#Scratch-directory-example" class="headerlink" title="Scratch directory example"></a>Scratch directory example</h2><p>We run a simple query and see what are the files generated in the scratch directory.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--@INTERNAL hive_version:hive2</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> col1&gt;<span class="number">0</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> col2</span><br></pre></td></tr></table></figure></p><ul><li><p>when query is submitted to the cluster and waiting for containers</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br></pre></td></tr></table></figure><ul><li>-ext- : a dir indicates the final query output</li><li>-mr- : a output directory for each MapReduce job</li><li>map.xml : map plan</li><li>reduce.xml : reduce plan</li></ul></li><li><p>when query is running in the hadoop cluster</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001/000000_0</span><br></pre></td></tr></table></figure><ul><li>data is generated in the -ext- dir  </li></ul></li><li>when query finished, scratch dir with all files are cleaned up</li></ul><h2 id="Scratch-directory-related-INFO-logs"><a href="#Scratch-directory-related-INFO-logs" class="headerlink" title="Scratch directory related INFO logs"></a>Scratch directory related INFO logs</h2><p>These info logs are generated when running the query in the previous section.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/_tmp_space.db</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020$/&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br><span class="line">exec.FileSinkOperator: Moving tmp dir: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001 to: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001</span><br></pre></td></tr></table></figure><ul><li><p>First several HDFS scratch directories are created during start <a href="(https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L698-L704">SessionState</a>.</p></li><li><p>_hive.hdfs.session.path = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}</p></li><li>hive.exec.plan = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}/${context execution id}-${task runner id}/-mr-${path id}/${random uuid}<ul><li><a href="https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L414" target="_blank" rel="noopener">hive.session.id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L313" target="_blank" rel="noopener">context execution id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L491" target="_blank" rel="noopener">task running id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L685" target="_blank" rel="noopener">path id</a></li><li><a href="https://github.com/apache/hive/blob/6d713b6564ecb9d1ae0db66c3742d2a8bc347211/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L668" target="_blank" rel="noopener">random uuid</a></li></ul></li><li>map.xml path = ${hive.exec.plan}/map.xml</li><li>reduce.xml path = ${hive.exec.plan}/reduce.xml</li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>How to fast fail hive jobs</title>
      <link href="/2018/11/15/hive-job-fast-fail/"/>
      <url>/2018/11/15/hive-job-fast-fail/</url>
      
        <content type="html"><![CDATA[<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>When running hive jobs in hadoop clusters on mapreduce, we always set the limitation of how much local and hdfs disk a job can use at most. Such limitation is per job basis and it can prevent a single job from using up too much disk resource and causing a node or a cluster unstable.</p><p>During running one job, if any task belong to the job finds the limitation is exceeded, the task will fail and at such time, we want to <strong>fast fail the job instead of retrying the failed tasks many times</strong>. Because since the local or hdfs max disk usage limitation is reached, the retry will probably still fail and is not much helpful.</p><h2 id="appoaches"><a href="#appoaches" class="headerlink" title="appoaches"></a>appoaches</h2><p>I will introduce how to configure to fast fail hive jobs when the local or hdfs limitation is exceeded.</p><h3 id="fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node"><a href="#fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node" class="headerlink" title="fast fail a job when too much local disk is used in one node"></a>fast fail a job when too much <em>local disk</em> is used in one node</h3><ul><li><p><strong>hadoop configuration</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.bytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>322122547200<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>implementation detail</strong><br>A task(org.apache.hadoop.mapred.Task) contains a thread checking all local working directories(mapreduce.cluster.local.dir), and will fast fail the job if any local working directory exceeds the limitation.</p></li><li><p><strong>required hadoop version</strong><br>3.1.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7022" target="_blank" rel="noopener">MAPREDUCE-7022</a></p></li></ul><h3 id="fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster"><a href="#fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster" class="headerlink" title="fast fail a job when too much hdfs disk is used in the cluster"></a>fast fail a job when too much <em>hdfs disk</em> is used in the cluster</h3><ul><li><p><strong>configuration</strong></p><ul><li><strong>hive configuration</strong><br>Set hive config hive.exec.scratchdir to per job basis, and set the quota limitation for the path.</li><li><strong>hadoop configuration</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>internal implementation</strong><br>A subclass of ClusterStorageCapacityExceededException will be thrown if the cluster capacity limitation is exceeded and then YarnChild will fast fail the job.</p></li><li><p><strong>required hadoop version</strong><br>3.3.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7148" target="_blank" rel="noopener">MAPREDUCE-7148</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
