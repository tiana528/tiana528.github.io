<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>HDFS UI detailed description</title>
      <link href="/2019/02/09/HDFS_UI_detailed_description/"/>
      <url>/2019/02/09/HDFS_UI_detailed_description/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS-UI"><a href="#HDFS-UI" class="headerlink" title="HDFS UI"></a>HDFS UI</h1><p>Open HDFS UI by browser, the URL is {hadoop_master_ip:50070}.</p><p>This article aims at explaining everything in the HDFS Overview UI in detail, other tabs content is self-described.</p><p>This article is based on hadoop of version 2.7.3.</p><p>(A lot of information can be get from FSNameSystem.)</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Started:Sun Feb 10 09:33:34 UTC 2019</span><br><span class="line">Version:2.7.3, ...</span><br><span class="line">Compiled:2018-11-16T09:23Z by root from ...</span><br><span class="line">Cluster ID:CID-c489b321-4d13-423b-a7e9-e8f66355e17a</span><br><span class="line">Block Pool ID:BP-5769292-172.18.204.140-1549791212026</span><br></pre></td></tr></table></figure><ul><li>Source code<ul><li><a href="https://github.com/apache/hadoop/blob/release-2.7.3-RC2/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.html" target="_blank" rel="noopener">dfshealth.html</a></li></ul></li><li>Explanation<ul><li>Started<ul><li>The starttime of the namenode.</li></ul></li><li>Version<ul><li>Hadoop version.</li></ul></li><li>Compiled<ul><li>Haodop compiled information.</li></ul></li><li>Cluster ID<ul><li>As explained in <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">hdfs federation</a>, a ClusterID identifier is used to identify all the nodes in the cluster. When a Namenode is formatted, this identifier is either provided or auto generated.</li><li>When formatting the namenode of an running hadoop cluster, a new ClusterID will be generated, it is also needed to change the <code>usr/local/hadoop/dfs/datanode/current/VERSION</code> file to update the value of ClusterID on all slaves. </li></ul></li><li>Block Pool ID:<ul><li>As explained in <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">hdfs federation</a>, a Block Pool is a set of blocks that belong to a single namespace.</li></ul></li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Security is off.</span><br><span class="line"></span><br><span class="line">Safemode is off.</span><br><span class="line"></span><br><span class="line">181 files and directories, 60 blocks = 241 total filesystem object(s).</span><br><span class="line"></span><br><span class="line">Heap Memory used 254 MB of 401.13 MB Heap Memory. Max Heap Memory is 9.94 GB.</span><br><span class="line"></span><br><span class="line">Non Heap Memory used 59.1 MB of 60.06 MB Commited Non Heap Memory. Max Non Heap Memory is -1 B.</span><br><span class="line"></span><br><span class="line">Configured Capacity:899.56 GB</span><br><span class="line">DFS Used:2.02 GB (0.23%)</span><br><span class="line">Non DFS Used:3.97 GB</span><br><span class="line">DFS Remaining:893.56 GB (99.33%)</span><br><span class="line">Block Pool Used:2.02 GB (0.23%)</span><br><span class="line">DataNodes usages% (Min/Median/Max/stdDev):0.06% / 0.31% / 0.31% / 0.12%</span><br><span class="line">Live Nodes3 (Decommissioned: 0)</span><br><span class="line">Dead Nodes0 (Decommissioned: 0)</span><br><span class="line">Decommissioning Nodes0</span><br><span class="line">Total Datanode Volume Failures0 (0 B)</span><br><span class="line">Number of Under-Replicated Blocks0</span><br><span class="line">Number of Blocks Pending Deletion2</span><br><span class="line">Block Deletion Start Time2/10/2019, 6:33:34 PM</span><br></pre></td></tr></table></figure><ul><li>Security<ul><li>Whether hadoop <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html" target="_blank" rel="noopener">security mode</a> is on or off. When Hadoop is configured to run in secure mode, each Hadoop service and each user must be authenticated by Kerberos.</li></ul></li><li>Safemode<ul><li>Whether <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Safemode" target="_blank" rel="noopener">safemode</a> is on or off. Safemode for the NameNode is essentially a read-only mode for the HDFS cluste</li><li></li></ul></li><li>181 files<ul><li>The current size of inodes in HDFS. <a href="https://github.com/apache/hadoop/blob/45caeee6cfcf1ae3355cd880402159cf31e94a8a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#L4844" target="_blank" rel="noopener">Link</a></li></ul></li><li>60 blocks<ul><li>The current number of blocks. <a href="https://github.com/apache/hadoop/blob/45caeee6cfcf1ae3355cd880402159cf31e94a8a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#L6017" target="_blank" rel="noopener">Link</a></li></ul></li><li>Memory related<ul><li>This part describes namenode memory information in the following format.<ul><li>used {used} MB of {committed} MB. Max Memory is {max}</li></ul></li><li>Refer to <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/management/MemoryUsage.html" target="_blank" rel="noopener">MemoryUsage</a> for the definition of used, committed, and max.</li><li>init is near xms, max is near xmx, used is actual being used, committed is garranteed by os. committed &gt;= used. There can be OutOfMemoryError if max &gt; used + allocating_memory &gt; committed, when os virtual memory is insufficient.</li></ul></li><li>Configured Capacity<ul><li>Total raw capacity of data nodes in bytes. <a href="https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java#L36" target="_blank" rel="noopener">Link</a></li></ul></li><li>DFS Used<ul><li>The percentage of the used capacity over the total capacity.</li></ul></li><li>Non DFS Used<ul><li>The total used space by data nodes for non-DFS purposes such as storing temporary files on the local file system.</li></ul></li><li>DFS Remaining<ul><li>The remaining capacity.</li></ul></li><li>Block Pool Used<ul><li>The used space by the block pool on all data nodes.</li></ul></li><li>DataNodes usages<ul><li>Min/Median/Max/stdDev refers to the minimum/median/maximum/standard_deviation of the dfs used space percentage by all datanodes.</li></ul></li><li>Live Nodes<ul><li>Number of datanodes which are currently live.</li></ul></li><li>Dead Nodes<ul><li>Number of datanodes which are currently dead.</li></ul></li><li>Decommissioning Nodes<ul><li>Number of datanodes where decommissioning is in progress.</li></ul></li><li>Total Datanode Volume Failures<ul><li>Total number of volume failures across all Datanodes.</li></ul></li><li>Number of Under-Replicated Blocks<ul><li>Get aggregated count of all blocks with low redundancy.</li></ul></li><li>Number of Blocks Pending Deletion<ul><li>The total number of blocks to be invalidated.</li></ul></li><li>Block Deletion Start Time<ul><li>The timestamp of bock deletion start.</li></ul></li></ul><h2 id="NameNode-Journal-Status"><a href="#NameNode-Journal-Status" class="headerlink" title="NameNode Journal Status"></a>NameNode Journal Status</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Current transaction ID: 278965</span><br><span class="line"></span><br><span class="line">Journal Manager: FileJournalManager(root=/.../hdfs/name)</span><br><span class="line">State : EditLogFileOutputStream(/.../hdfs/name/current/edits_inprogress_0000000000000000001)</span><br></pre></td></tr></table></figure><ul><li>Current transaction ID<ul><li>The last transaction ID that was either loaded from an image or loaded by loading edits files. Note that this is not precise value and can only be used by metrics.</li></ul></li><li>Journal Manager<ul><li>The class of Jourmnal Manager</li><li>A <a href="https://github.com/apache/hadoop/blob/fac9f91b2944cee641049fffcafa6b65e0cf68f2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java" target="_blank" rel="noopener">JournalManager</a> is responsible for managing a single place of storing edit logs. It may correspond to multiple files, a backup node, etc. Even when the actual underlying storage is rolled, or failed and restored, each conceptual place of storage corresponds to exactly one instance of this class, which is created when the EditLog is first opened.</li></ul></li><li>State:<ul><li>A short text snippet suitable for describing the current status of the EditLogOutputStream(EditLogOutputStream is for supporting journaling of edits logs into a persistent storage).</li></ul></li></ul><h2 id="NameNode-Storage"><a href="#NameNode-Storage" class="headerlink" title="NameNode Storage"></a>NameNode Storage</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NameNode Storage</span><br><span class="line">Storage Directory : /.../hdfs/name</span><br><span class="line">Type : IMAGE_AND_EDITS</span><br><span class="line">State : Active</span><br></pre></td></tr></table></figure><ul><li>Storage Directory<ul><li>The storage directory for namenode</li></ul></li><li>Type<ul><li>StorageDirType specific to namenode storage. A Storage directory could be of type IMAGE which stores only fsimage, or of type EDITS which stores edits or of type IMAGE_AND_EDITS which stores both fsimage and edits.</li></ul></li><li>State<ul><li>Status of namenode.</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Basic understanding of debian packaging</title>
      <link href="/2019/02/07/Basic_understanding_of_debian_packaging/"/>
      <url>/2019/02/07/Basic_understanding_of_debian_packaging/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>It never becomes easy to setup/install/upgrade environments. This article aims at describing how to use/create debian packages to help setup the environments.</p><h2 id="What-is-a-debian-package"><a href="#What-is-a-debian-package" class="headerlink" title="What is a debian package"></a>What is a debian package</h2><ul><li>Linux<ul><li><a href="https://en.wikipedia.org/wiki/Linux" target="_blank" rel="noopener">Linux</a> is a family of free and open-source software operating systems based on the Linux kernel.</li></ul></li><li>Linux kernel<ul><li><a href="https://en.wikipedia.org/wiki/Linux_kernel" target="_blank" rel="noopener">Linux kernel</a> is a free and open-source, monolithic, Unix-like operating system kernel.</li></ul></li><li>Linux distribution<ul><li>roughly : Linux distribution = Linux kernel + software</li><li>A <a href="https://en.wikipedia.org/wiki/Linux_distribution" target="_blank" rel="noopener">Linux distribution</a> is an operating system made from a software collection, which is based upon the Linux kernel and, often, a package management system. There are more than 600 kinds of linux distributions.</li></ul></li><li>Debian<ul><li>Debian is one of the Linux distributions, also called Debian GNU/Linux.</li><li><a href="https://en.wikipedia.org/wiki/Debian" target="_blank" rel="noopener">Debian</a> is one of the earliest operating systems based on the Linux kernel, and containing more than 51000 <a href="https://www.debian.org/index.en.html" target="_blank" rel="noopener">debian packges</a></li></ul></li><li>Debian package<ul><li>A <a href="https://wiki.debian.org/Packaging" target="_blank" rel="noopener">Debian package</a> is a collection of files that allow for applications or libraries to be distributed via the Debian package management system. </li></ul></li><li>Relationship between debian packages and linux distributions<ul><li><a href="https://en.wikipedia.org/wiki/List_of_Linux_distributions" target="_blank" rel="noopener">Linux distributions</a> can be divided into several categories according to how packages are managed. PRM-based(.rpm file) distributions includes Red Hat Linux, CentOS, Fedora and so on. Debian-based(.deb file) distributions includes Ubuntu, GNU/Linux and so on.</li></ul></li></ul><h1 id="Create-debian-packages"><a href="#Create-debian-packages" class="headerlink" title="Create debian packages"></a>Create debian packages</h1><p>It is possible to use the official tool(dpkg-deb) to create debian packages. But here we use <a href="https://github.com/jordansissel/fpm" target="_blank" rel="noopener">fpm</a> to create debian packages. It is very simple to use.</p><p>Refer <a href="https://fpm.readthedocs.io/en/latest/installing.html" target="_blank" rel="noopener">this</a> to install fpm.</p><h3 id="Example1"><a href="#Example1" class="headerlink" title="Example1"></a>Example1</h3><p>Create a debian package whose name is “hello_world”, and what it do is installing a file “hello world” in the path of /tmp/hi.txt</p><p>It is very easy, just execute 3 commands.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p working_folder/tmp</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world"</span> &gt; working_folder/tmp/hi.txt</span><br><span class="line">fpm \</span><br><span class="line">  --architecture=amd64 \</span><br><span class="line">  --<span class="built_in">chdir</span>=working_folder \</span><br><span class="line">  --input-type=dir \</span><br><span class="line">  --output-type=deb \</span><br><span class="line">  --name=hello_world \</span><br><span class="line">  --version=<span class="string">"0.0.1"</span> \</span><br><span class="line">  --iteration=<span class="string">"001"</span> \</span><br><span class="line">  --description=<span class="string">"This is an example"</span> \</span><br><span class="line">  --deb-user=<span class="string">"root"</span> \</span><br><span class="line">  --deb-group=<span class="string">"root"</span> \</span><br><span class="line">  --deb-priority=<span class="string">"extra"</span> \</span><br><span class="line">  <span class="string">"."</span></span><br></pre></td></tr></table></figure><p>Then a file of hello-world_0.0.1-001_amd64.deb will be created in the current folder.<br>Use <code>sudo dpkg -i hello-world_0.0.1-001_amd64.deb</code> to install the deb file, and then you can see a file(/tmp/hi.txt) containing “hello world” is created.</p><h2 id="Example2"><a href="#Example2" class="headerlink" title="Example2"></a>Example2</h2><p>This is an example to create a symlink /etc/link which links to /tmp/hi.txt. This example shows the usage of <code>--after-install</code> to do something at the end of package installation.</p><p>Create a file hook-AfterInstall.sh.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">ln -s /tmp/hi.txt /etc/link</span><br></pre></td></tr></table></figure></p><p>Then run 3 commands.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p working_folder/tmp</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world"</span> &gt; working_folder/tmp/hi.txt</span><br><span class="line">fpm \</span><br><span class="line">  --architecture=amd64 \</span><br><span class="line">  --<span class="built_in">chdir</span>=working_folder \</span><br><span class="line">  --input-type=dir \</span><br><span class="line">  --output-type=deb \</span><br><span class="line">  --name=hello_world \</span><br><span class="line">  --version=<span class="string">"0.0.1"</span> \</span><br><span class="line">  --iteration=<span class="string">"002"</span> \</span><br><span class="line">  --description=<span class="string">"This is an example"</span> \</span><br><span class="line">  --deb-user=<span class="string">"root"</span> \</span><br><span class="line">  --deb-group=<span class="string">"root"</span> \</span><br><span class="line">  --deb-priority=<span class="string">"extra"</span> \</span><br><span class="line">  --after-install=<span class="string">"hook-AfterInstall.sh"</span> \</span><br><span class="line">  <span class="string">"."</span></span><br></pre></td></tr></table></figure></p><p>Then hello-world_0.0.1-002_amd64.deb is created in the current folder. Installing it will create a symlink.</p><h1 id="Debian-package-management-tools"><a href="#Debian-package-management-tools" class="headerlink" title="Debian package management tools"></a>Debian package management tools</h1><h2 id="dpkg"><a href="#dpkg" class="headerlink" title="dpkg"></a>dpkg</h2><ul><li>See help<ul><li>dpkg –help</li><li>dpkg –force-help</li></ul></li><li>Print out the control file and other information for a specified package<ul><li>dpkg –info foo.deb</li></ul></li><li>Install a package onto the file system of the hard disk<ul><li>dpkg –install foo_VVV-RRR.deb</li></ul></li><li>List the files in a package deb file<ul><li>dpkg -c foo.deb</li></ul></li><li>Remove a package (but not its configuration files)<ul><li>dpkg –remove foo</li></ul></li><li>Remove a package (including its configuration files)<ul><li>dpkg –purge foo</li></ul></li><li>Remove a package no matter that it is depended by other existing packages<ul><li>dpkg –purge –force-depends foo</li></ul></li><li>List all files ‘owned’ by a package<ul><li>dpkg –listfiles foo</li></ul></li><li>Find packages by file name.<ul><li>dpkg –search foo*</li></ul></li><li>Find packages by package name.<ul><li>dpkg –list foo*</li></ul></li></ul><h2 id="aptitude"><a href="#aptitude" class="headerlink" title="aptitude"></a>aptitude</h2><p>aptitude provides the functionality of apt-get</p><ul><li>Install a new package<ul><li>apt-get install foo</li></ul></li><li>Install a new package by force<ul><li>apt-get -f install</li></ul></li><li>Synchronize the package index files from their sources<ul><li>apt-get update</li></ul></li><li>Remove packages without configurations<ul><li>apt-get remove foo</li></ul></li><li>Remove packages with configurations<ul><li>apt-get purge foo</li></ul></li><li>Search packages<ul><li>apt-cache search foo<ul><li>Lists packages whose name or description contains foo</li><li>This queries and displays available information about installed and installable packages.</li></ul></li></ul></li></ul><h2 id="dpkg-deb"><a href="#dpkg-deb" class="headerlink" title="dpkg-deb"></a>dpkg-deb</h2><ul><li>Determine what files are contained in a Debian archive file<ul><li>dpkg-deb –contents foo.deb</li></ul></li></ul><h1 id="Common-questions"><a href="#Common-questions" class="headerlink" title="Common questions"></a>Common questions</h1><ul><li>What are <a href="https://www.debian.org/doc/manuals/maint-guide/dother.en.html#conffiles" target="_blank" rel="noopener">conffiles</a>?<ul><li>conffiles are just configuration files of a package. When you upgrade a package, you’ll be asked whether you want to keep your old configuration files or not.</li><li>Files under the /etc directory are marked as conffiles automatically.</li><li>If your program uses configuration files but also rewrites them on its own, it’s best not to make them conffiles because dpkg will then prompt users to verify the changes all the time.</li><li>If the program you’re packaging requires every user to modify the configuration files in the /etc directory, there are two popular ways to arrange for them to not be conffiles, keeping dpkg quiet:<ul><li>Create a symlink under the /etc directory pointing to a file under the /var directory generated by the maintainer scripts.</li><li>Create a file generated by the maintainer scripts under the /etc directory.</li></ul></li></ul></li><li>What packages are already installed?<ul><li>dpkg –list</li><li>dpkg –list ‘foo*’</li></ul></li><li>How to check detail of installed packages<ul><li>dpkg –status foo</li></ul></li><li>How to display the files on an installed package?<ul><li>dpkg –listfiles foo</li></ul></li><li>How can I find out what package produced a particular file?<ul><li>dpkg –search foo<ul><li>This will search all of the files having the file extension of .list in the directory /var/lib/dpkg/info/.</li></ul></li></ul></li><li>Can multiple versions of the same package exist on the same system?<ul><li>No.</li></ul></li><li>What happens when upgrade a binary debian package?<ul><li>It will first delete all files created by the old version except conffiles, then install the new version. So there might be a small period that there will be no available/executable package.</li></ul></li><li>What are conffiles of an installed packages?<ul><li>ls /var/lib/dpkg/info/. | grep conffiles</li></ul></li><li>How to install packages in place without breaking existing running service?<ul><li>As we discribed before, updating a package will first delete all files owned by the current installed version, then install the new version. There is a small time window during the package versionup that there is no available/executable package. This should be considered when performing environment update. One solution is rolling update, never let a service in the middle of update provides actual service. The other is always installing new packages and do not update existing packages.</li></ul></li><li>How to downgrade the version of a package?<ul><li>apt get install foo=1.2 –allow-downgrades</li></ul></li><li>How to dealwith conffiles during package version up?<ul><li><a href="https://raphaelhertzog.com/2010/09/21/debian-conffile-configuration-file-managed-by-dpkg/" target="_blank" rel="noopener">Everything you need to know about conffiles: configuration files managed by dpkg</a></li></ul></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://www.debian.org/doc/manuals/debian-faq/ch-pkgtools.en.html" target="_blank" rel="noopener">FAQ</a> of dpkg tools</li><li>How to use <a href="https://help.ubuntu.com/community/AptGet/Howto" target="_blank" rel="noopener">apt-get</a>?</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> debian package </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop cluster incidents</title>
      <link href="/2019/01/25/hadoop-cluster-incidents/"/>
      <url>/2019/01/25/hadoop-cluster-incidents/</url>
      
        <content type="html"><![CDATA[<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><p>The operation of Hadoop cluster is not easy. There are many parameters and there is no perfect configuration that fit all situations. Monitoring and learn from each incidents is very important. This article aims at summarizing some incidents and they can be seen as valued knowledge to help investigate future incidents and discuss approaches to prevent. </p><h1 id="datanode-Xmx-configured-does-not-fit-the-instance"><a href="#datanode-Xmx-configured-does-not-fit-the-instance" class="headerlink" title="datanode Xmx configured does not fit the instance"></a>datanode Xmx configured does not fit the instance</h1><ul><li>situation<ul><li>The whole cluster performance degrades.</li><li>One node in the cluster has extremely higher CPU utilization than other nodes.</li><li>The datanode process on that node takes up more than 300% CPU by checking top command.</li><li>The accumulate GC time of the datanode processs on that node increases rapidly(know it through datadog jvm.gc.parnew.time metrics).</li></ul></li><li>analysis<ul><li>The cluster performance degrades because one datanode process behaves bad.</li><li>GC time increases rapidly indicates the memory is insufficient.</li></ul></li><li>root cause<ul><li>The Xmx parameter of the datanode process is configured too large. The slave instance type is aws c4.2xlarge which is 15GB, 8vCPU. Each core(map,reduce,application master) Xmx is configured as 2GB, node manager Xmx is also 2GB, but datanode JMX is configured as 10GB. Meaning that the datanode Xmx configured is much higher than the free memory on the node thus causing frequent memory swap and performance degradation, as well as longer GC time. This articles explains a little more about <a href="http://www.javaperformancetuning.com/news/qotm045.shtml" target="_blank" rel="noopener">the situation when a large heap(Xmx) is configured</a>.</li></ul></li><li>learn<ul><li>Monitoring GC time increasing speed helps find the situations that the JVM paramers are mis-configured. Free space on the node should be considered when deciding the Xmx value.</li></ul></li></ul><h1 id="to-be-continued"><a href="#to-be-continued" class="headerlink" title="to be continued"></a>to be continued</h1>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Very basic understanding of CAP theorem</title>
      <link href="/2019/01/19/basic-understanding-of-cap-theorem/"/>
      <url>/2019/01/19/basic-understanding-of-cap-theorem/</url>
      
        <content type="html"><![CDATA[<h1 id="Wiki-description"><a href="#Wiki-description" class="headerlink" title="Wiki description"></a>Wiki description</h1><p>According to the <a href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener">wiki</a>: CAP theorem explains that it is impossible for a <em>distributed data store</em> to simultaneously provide more than two out of the following three guarantees:</p><ul><li>Consistency: Every read receives the most recent write or an error.</li><li>Availability: Every request receives a (non-error) response – without the guarantee that it contains the most recent write.</li><li>Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.</li></ul><h1 id="Simple-understanding"><a href="#Simple-understanding" class="headerlink" title="Simple understanding"></a>Simple understanding</h1><p>Here explain the theorem using a simple example. Considering there are two nodes, and data is replicated in two nodes.</p><ul><li>If AP, then not C. When network partition happens(P), a write request arrives at one node and success(A), then the data on the two nodes must be inconsistent, meaning C is not satisfied.</li><li>If CP, then not A. When network partition happens(P), the read on either node always gets the latest written value. Then write operation must be forbidden on both side of the parititons, meaning A is not satisfied.</li><li>IF AC, then not P. If write always successes(A), read always gets the recent written value(C), then network partition must not happen, meaning P is not satisfied.</li></ul><h1 id="Does-it-explain-well"><a href="#Does-it-explain-well" class="headerlink" title="Does it explain well"></a>Does it explain well</h1><p>The above understanding of CAP is not sufficient. CAP is always criticized for being too simplistic and often misleading. Actually, many distributed data systems are not even CP, AP or AC, considering the strict definition. It is difficult for people to understand what is CAP by the above simple description which is explained in a lot of places on the Internet. The following of this article aims at <strong>adding a little more about the missing part of the above CAP theorem description and make it understandable</strong>.</p><h1 id="Definition-of-terms"><a href="#Definition-of-terms" class="headerlink" title="Definition of terms"></a>Definition of terms</h1><p>Before going any further, we need to make precise definition of terms. There are many articles discussing CAP, without clear definition and the readers may not have enough knowledge to understand them.</p><h2 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h2><ul><li>serialization is a transaction concept. Multiple transactions are processed in parallel, and each transaction contains one or more instructions. The database needs to <a href="https://en.wikipedia.org/wiki/Schedule_(computer_science)" title="[external] [title]" target="_blank" rel="noopener">schedule</a> how the instructions are executed. Note that different execution orders may result in different result.<ul><li>For example, start with the status a = 1, b = 3<ul><li>t1<ul><li>i1 : t1 = a</li><li>i2 : b = t1 + 1</li></ul></li><li>t2<ul><li>i3 : t2 = b</li><li>i4 : a = t2 + 1</li></ul></li><li>Execution order of i1, i3, i2, i4 results in a = 4, b = 2. Execution order of i1, i2, i3, i4 results in a = 3, b = 2.</li></ul></li></ul></li><li>A transaction schedule is <em>serializable</em> if the outcome is as if transactions are executed atomically and in some sequential order.</li><li>Strict serializability : Serializability + transactions are processed in the “real-time” order.</li></ul><h2 id="How-to-garrantee-serialization"><a href="#How-to-garrantee-serialization" class="headerlink" title="How to garrantee serialization"></a>How to garrantee serialization</h2><ul><li>Using concurrency control protocol  such as <a href="https://en.wikipedia.org/wiki/Two-phase_locking" target="_blank" rel="noopener">2PL</a><ul><li>2PL : 2 phrase locking<ul><li>acquire locks in first phrase, release locks in second phrase</li></ul></li><li>C2PL : Conservative two-phase locking<ul><li>acquire all locks at the beginning of the transaction, release locks in the second phrase.</li></ul></li><li>S2PL : Strict two-phase locking<ul><li>acquire locks in first phrase, release write locks at the end of the  transaction.</li></ul></li><li>SS2PL : Strong strict two-phase locking<ul><li>acquire locks in first phrase, release both read and write locks at the end of the transaction.</li><li>SS2PL has been the concurrency control protocol of choice for most database systems and utilized since their early days in the 1970s.</li></ul></li></ul></li><li>Using other approaches</li></ul><h2 id="Linearizability"><a href="#Linearizability" class="headerlink" title="Linearizability"></a>Linearizability</h2><ul><li><a href="https://en.wikipedia.org/wiki/Linearizability" target="_blank" rel="noopener">Linearizability</a> concept is from concurrent programming.  Linearizability is originally talking about single objects and not transactions. Nowadays, many papers talk about linearilzability transactions the same as strict serializability, and each transaction contains only one operation. </li><li>How Linearizability is related with CAP<ul><li>Linearizability is what the CAP Theorem calls Consistency.</li></ul></li></ul><h2 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h2><ul><li><a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)" title="[external] [title]" target="_blank" rel="noopener">Phantom Read Phenomena</a><ul><li>Dirty reads<ul><li>A dirty read occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.</li></ul></li><li>Non-repeatable reads<ul><li>A non-repeatable read occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.</li></ul></li><li>Phantom reads<ul><li>A phantom read occurs when, in the course of a transaction, new rows are added or removed by another transaction to the records being read.</li></ul></li><li>Note that only these are insufficient and there are more situations.</li></ul></li><li>Levels(stronger to lower)<ul><li>Serializable<ul><li>This is the highest isolation level.</li><li>With a lock-based concurrency control DBMS implementation, serializability requires read and write locks (acquired on selected data) to be released at the end of the transaction. Also range-locks must be acquired when a SELECT query uses a ranged WHERE clause, especially to avoid the phantom reads phenomenon.</li></ul></li><li>Repeatable reads<ul><li>a lock-based concurrency control DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. However, range-locks are not managed, so phantom reads can occur.</li></ul></li><li>Read committed<ul><li>a lock-based concurrency control DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the SELECT operation is performed (so the non-repeatable reads phenomenon can occur in this isolation level). As in the previous level, range-locks are not managed.</li></ul></li><li>Read uncommitted<ul><li>This is the lowest isolation level. In this level, dirty reads are allowed, so one transaction may see not-yet-committed changes made by other transactions.</li></ul></li><li>There are also incufficient and there are more that can be listed.</li></ul></li><li>How isolation is related in CAP<ul><li>Since the C(consistency) in CAP refers to linearizability which is very strong, and per transaction contains one operation, all actions happen instantaneously. The C(consistency) actually ensures the strongest isolation : serializable isolution level.</li></ul></li></ul><h2 id="What-isolation-level-are-commercial-databases-providing"><a href="#What-isolation-level-are-commercial-databases-providing" class="headerlink" title="What isolation level are commercial databases providing"></a>What isolation level are commercial databases providing</h2><ul><li>READ COMMITTED is defaulted isolation level on PostgreSQL, SQL Server, and Oracle.</li><li>REPEATABLE READ is defaulted isolation level on Mysql Innodb.</li></ul><p>We can see that serializable isolation level is not used at lease by default. It is too heavy and required more locks(If use lock as concurrency control), less concurrency, and less throughput. It also infers that in CAP, even if we can only choose 2 of them at the same time, C(strong consistency, linearizability, highest isolation level) is mostly not chosen. Most databases choose weaker consistency, which has better performance.</p><h1 id="Look-back-at-CAP-theorem"><a href="#Look-back-at-CAP-theorem" class="headerlink" title="Look back at CAP theorem"></a>Look back at CAP theorem</h1><ul><li>C<ul><li>definition<ul><li>A guarantee that every node in a distributed cluster returns the same, most recent, successful write. Consistency refers to every client having the same view of the data. There are various types of consistency models. Consistency in CAP (used to prove the theorem) refers to linearizability or sequential consistency, a very strong form of consistency.</li></ul></li><li>meaning<ul><li>Apparently, this is very strong consistency and there are no dirty reads, non-repeatable reads, or phantom reads. </li></ul></li></ul></li><li>A<ul><li>definition<ul><li>Every non-failing node returns a response for all read and write requests in a reasonable amount of time. The key word here is every. To be available, every node on (either side of a network partition) must be able to respond in a reasonable amount of time.</li></ul></li><li>meaning<ul><li>If some operation finishes exceeding an acceptable time(e.g. 30 seconds), it just means not available.</li><li>This definition of availability is also strong. Note that during network partition, even if you say the partition which contains the majority of nodes responses the client, it is not seen as achieve availability in CAP. It is called availability if the less node partition nodes also responses write/read to the client without error.</li></ul></li></ul></li><li>P<ul><li>definition<ul><li>The system continues to function and upholds its consistency guarantees in spite of network partitions. Network partitions are a fact of life. Distributed systems guaranteeing partition tolerance can gracefully recover from partitions once the partition heals.</li></ul></li><li>meaning<ul><li>This is unavoided in a distributed environment. Not only network parition happens, but also network connection timeout can be seen as parititon.</li></ul></li></ul></li></ul><h1 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h1><p>In a network partition, even if it is only posssible to make a choice between avaialbility or consistency. Many databases are not choosing any of them, since the consistency (linearizability) is so expensive. Actually, this leaves a space for the database designers/users to choose a level of balanced consistency and availability according to the use case. Whenever looking into a database, it will be a good point to check what level of A and C does it provides when P happens, even if it is not perfect A or C according to the CAP definition.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html" target="_blank" rel="noopener">please-stop-calling-databases-cp-or-ap</a></li><li><a href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="noopener">cap-twelve-years-later-how-the-rules-have-changed</a></li><li><a href="https://begriffs.com/posts/2017-08-01-practical-guide-sql-isolation.html" target="_blank" rel="noopener">practical-guide-sql-isolation</a></li><li><a href="https://irenezhang.net/blog/2015/02/01/consistency.html" target="_blank" rel="noopener">consistency</a></li><li><a href="https://project.inria.fr/epfl-Inria/files/2017/02/JadHamza-talk.pdf" target="_blank" rel="noopener">What is the strongest consistency that could be achieved<br>in a large scale distributed system?</a></li><li><a href="http://www.bailis.org/blog/linearizability-versus-serializability/" target="_blank" rel="noopener">linearizability-versus-serializability/</a></li><li><a href="https://dddpaul.github.io/blog/2016/03/17/linearizability-and-serializability/" target="_blank" rel="noopener">linearizability-and-serializability/</a></li><li><a href="https://cs.stackexchange.com/questions/41698/linearizability-and-serializability-in-context-of-software-transactional-memory" target="_blank" rel="noopener">linearizability-and-serializability-in-context-of-software-transactional-memory</a></li><li><a href="https://dzone.com/articles/understanding-the-cap-theorem" target="_blank" rel="noopener">understanding-the-cap-theorem</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive GenericUDF and DeferredJavaObject analysis</title>
      <link href="/2019/01/09/hive-DeferredJavaObject/"/>
      <url>/2019/01/09/hive-DeferredJavaObject/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>This article aims at discussing how hive generic User-defined function(GenericUDF) works. In the java doc, it says GenericUDF can do short-circuit evaluations using DeferedObject. But what is short-circuit evaluation and how DeferedObject works?</p><h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Basicly, use GenericUDF when the input or output is complex type, or input arguments have variable length.</p><p>GenericUDF use DeferedObject to pass arguments and achieve lazy-evaluation and short-circuiting.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">interface</span> <span class="title">DeferredObject</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">prepare</span><span class="params">(<span class="keyword">int</span> version)</span> <span class="keyword">throws</span> HiveException</span>;</span><br><span class="line">  <span class="function">Object <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> HiveException</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h1 id="Source-code-analysis"><a href="#Source-code-analysis" class="headerlink" title="Source code analysis"></a>Source code analysis</h1><p>We run a sql <code>select abs(c1) from t1</code> and try to analysis how UDF is initialized and processed.</p><ul><li>Class structure and relationship<ul><li>SelectOperator<ul><li>ExprNodeEvaluator[] eval<ul><li>(actually it is ExprNodeGenericFuncEvaluator)</li></ul></li></ul></li><li>ExprNodeGenericFuncEvaluator<ul><li>ExprNodeEvaluator[] children; <ul><li>(actually it is [ExprNodeColumnEvaluator[Column[c1]]])</li></ul></li><li>GenericUDF.DeferredObject[] deferredChildren;<ul><li>(actually it is org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject and contains the ExprNodeEvaluator)</li></ul></li><li>GenericUDF.DeferredObject[] childrenNeedingPrepare;<ul><li>(actually it contains the same object as deferredChildren in this example)</li></ul></li><li>GenericUDF genericUDF;<ul><li>(actually it is GenericUDFAbs)</li></ul></li></ul></li></ul></li></ul><ul><li><p>During initialization</p><ul><li>Generate query plan tree<ul><li>SemanticAnalyzer.genPlan</li></ul></li><li><p>Create ExprNodeGenericFuncDesc based on genericUDFClass</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExprNodeGenericFuncDesc <span class="title">newInstance</span><span class="params">(GenericUDF genericUDF,</span></span></span><br><span class="line"><span class="function"><span class="params">      String funcText,</span></span></span><br><span class="line"><span class="function"><span class="params">      List&lt;ExprNodeDesc&gt; children)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    ObjectInspector oi = genericUDF.initializeAndFoldConstants(childrenOIs);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Initialize GenericUDF</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.udf.generic.GenericUDF</span><br><span class="line"><span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initializeAndFoldConstants</span><span class="params">(ObjectInspector[] arguments)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line"></span><br><span class="line">    ObjectInspector oi = initialize(arguments);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Initial select operator</p><ul><li><p>Create ExprNodeGenericFuncEvaluator</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.SelectOperator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initializeOp</span><span class="params">(Configuration hconf)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">  ...</span><br><span class="line">  eval[i] = ExprNodeEvaluatorFactory.get(colList.get(i), hconf);</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ExprNodeGenericFuncEvaluator</span><span class="params">(ExprNodeGenericFuncDesc expr, Configuration conf)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    children = <span class="keyword">new</span> ExprNodeEvaluator[expr.getChildren().size()];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; children.length; i++) &#123;</span><br><span class="line">      ExprNodeDesc child = expr.getChildren().get(i);</span><br><span class="line">      ExprNodeEvaluator nodeEvaluator = ExprNodeEvaluatorFactory.get(child, conf);</span><br><span class="line">      children[i] = nodeEvaluator;</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Init ExprNodeGenericFuncEvaluator and create DeferredExprObject</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector rowInspector)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">     deferredChildren = <span class="keyword">new</span> GenericUDF.DeferredObject[children.length];</span><br><span class="line">     </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExprNodeGenericFuncEvaluator.DeferredExprObject</span><br><span class="line"> DeferredExprObject(ExprNodeEvaluator eval, <span class="keyword">boolean</span> eager) &#123;</span><br><span class="line">      <span class="keyword">this</span>.eval = eval; <span class="comment">//(ExprNodeEvaluator[Column[c1]])</span></span><br><span class="line">      <span class="keyword">this</span>.eager = eager; (<span class="keyword">false</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>During processing data</p><ul><li><p>select operator process one row</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.SelectOperator</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object row, <span class="keyword">int</span> tag)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    output[i] = eval[i].evaluate(row); <span class="comment">//(eval[i] is actually ExprNodeGenericFuncEvaluator)</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Note that <code>row</code> is an LazyStruct object which holds data in binary format</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Object <span class="title">_evaluate</span><span class="params">(Object row, <span class="keyword">int</span> version)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> genericUDF.evaluate(deferredChildren);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    Object valObject = arguments[<span class="number">0</span>].get();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Note that arguments[0] is ExprNodeGenericFuncEvaluator&amp;DeferredExprObject</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">     ...</span><br><span class="line">     obj = eval.evaluate(rowObject, version); <span class="comment">//(eval is ExprNodeColumnEvaluator)</span></span><br><span class="line">     ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator</span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Object <span class="title">_evaluate</span><span class="params">(Object row, <span class="keyword">int</span> version)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> inspector.getStructFieldData(row, field); <span class="comment">//(inspector is LazySimpleStructObjectInspector)</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h1><ul><li>Processing<ul><li>During intilization<ul><li>hive creates SelectOperator, SelectOperator contains ExprNodeGenericFuncEvaluator, ExprNodeGenericFuncEvaluator contains GenericUDFAbs and DeferredExprObject (DeferredObject), DeferredExprObject contains ExprNodeColumnEvaluator (column[c1]).</li></ul></li><li>During executing<ul><li>SelectOperator processes row which is LazyStruct, and at last passed to LazySimpleStructObjectInspector (getStructFieldData) to get the actual data from binary data.</li></ul></li></ul></li><li>lazy-evaluation and short-circuiting<ul><li>We can notice that the value of the attribute which is involved in the UDF calculation is only evaluated just before use. The value is analyzed from binary format. This is very efficient.</li></ul></li></ul><h1 id="Cache-approach-in-GenericUDF"><a href="#Cache-approach-in-GenericUDF" class="headerlink" title="Cache approach in GenericUDF"></a>Cache approach in GenericUDF</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    Object obj = arguments[<span class="number">0</span>].get();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The obj got DeferredObject in GenericUDF evaluate function is  an LazyInteger object, it is always the same object even if evaluate function is invoked for processing a different row.</p><p>We should be careful when making use of cache to speedup GenericUDF calculation. Buffer the input and output, and returns buffered output if historical input comes. Since we always get the same object from the DeferredObject, if we use <code>equals</code> function to compare, it will always be true. We need to deep copy the inputs for further comparison. </p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>create git hooks in mac</title>
      <link href="/2018/12/15/create_git_hooks_in_mac/"/>
      <url>/2018/12/15/create_git_hooks_in_mac/</url>
      
        <content type="html"><![CDATA[<p>This article explains how to create git hooks in mac, and how to use the customized git hook chains.</p><h2 id="Preparation-for-creating-hook-chain"><a href="#Preparation-for-creating-hook-chain" class="headerlink" title="Preparation for creating hook chain"></a>Preparation for creating hook chain</h2><p>(This part refers to the <a href="https://stackoverflow.com/questions/8730514/chaining-git-hooks/8734391#8734391" target="_blank" rel="noopener">hook-chain</a>, and make some modification for running in mac.)<br></p><p>Motivation is that when you run git commit, the pre-commit script will be invoked before commit operation actually happen. If you want to do multiple checks in the pre-commit phrase, instead of putting all logic in one file of pre-commit, a better idea is separating the check logic in separated files, and trigger each check one by one as a chain.</p><ul><li><p>Create git templates hooks folder</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/.git-templates/hooks</span><br><span class="line"><span class="built_in">cd</span> ~/.git-templates/hooks</span><br></pre></td></tr></table></figure><p>All files in this folder will be copied to each git project’s hook folder when running git init. </p></li><li><p>Create hook-chain file under the hooks folder</p><p>file name : hook-chain</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">hookname=`basename <span class="variable">$0</span>`</span><br><span class="line">git_folder=`dirname <span class="variable">$0</span>`</span><br><span class="line"><span class="keyword">for</span> hook <span class="keyword">in</span> $(ls <span class="variable">$git_folder</span>/<span class="variable">$hookname</span>.*)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$hook</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"executing hook : <span class="variable">$hook</span>"</span></span><br><span class="line">        sh <span class="string">"<span class="variable">$hook</span>"</span></span><br><span class="line">        status=$?</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">test</span> <span class="variable">$status</span> -ne 0; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"Hook <span class="variable">$hook</span> failed with error code <span class="variable">$status</span>"</span></span><br><span class="line">            <span class="built_in">exit</span> <span class="variable">$status</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>What this do is executing all hook files whose name begin with the current git hook. For example, when invoking git hook of pre-commit, this will invoke all scripts starting with pre-commit, such as pre-commit.json, pre-commit.yaml, etc.</p></li><li><p>Create hook files</p><p>Refer to <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank" rel="noopener">git hooks</a> for all client side git hooks and their meaning. For example, pre-rebase, pre-commit, post-checkout, post-merge, pre-push, etc. Create the ones that you need under ~/.git-templates/hooks.</p><p>In my case, I want to create pre-commit hook, so I run:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s hook-chain pre-commit</span><br></pre></td></tr></table></figure><p>Then I will also create pre-commit.json, pre-commit.yaml, pre-commit.awskey in the same folder as explained in the following sessions. When pre-commit is invoked, it will trigger all these three files of pre-commit.json, pre-commit.yaml and pre-commit.awskey one by one.</p><p>Note that each hook file should be executable.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x ~/.git-templates/hooks/.</span><br></pre></td></tr></table></figure></li></ul><h2 id="Write-customized-hooks"><a href="#Write-customized-hooks" class="headerlink" title="Write customized hooks"></a>Write customized hooks</h2><h3 id="Prevent-commit-the-aws-access-key-and-secret-key"><a href="#Prevent-commit-the-aws-access-key-and-secret-key" class="headerlink" title="Prevent commit the aws access key and secret key"></a>Prevent commit the aws access key and secret key</h3><p>(script is <a href="https://gist.github.com/saliceti/7eb0ba0bb5ed875df515" target="_blank" rel="noopener">from</a>)</p><ul><li>file name : pre-commit.awskey<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install globally using https://coderwall.com/p/jp7d5q/create-a-global-git-commit-hook</span></span><br><span class="line"><span class="comment"># The checks are simple and can give false positives. Amend the hook in the specific repository.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> git rev-parse --verify HEAD &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    against=HEAD</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="comment"># Initial commit: diff against an empty tree object</span></span><br><span class="line">    EMPTY_TREE=$(git <span class="built_in">hash</span>-object -t tree /dev/null)</span><br><span class="line">    against=<span class="variable">$EMPTY_TREE</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redirect output to stderr.</span></span><br><span class="line"><span class="built_in">exec</span> 1&gt;&amp;2</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Check changed files for an AWS keys</span></span><br><span class="line">FILES=$(git diff --cached --name-only <span class="variable">$against</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$FILES</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    KEY_ID=$(grep -rE --line-number <span class="string">'(^|[^A-Za-z0-9/+=])AKIA[A-Z0-9]&#123;16&#125;($|[^A-Za-z0-9/+=])'</span> <span class="variable">$FILES</span>)</span><br><span class="line">    KEY=$(grep -rE --line-number <span class="string">'(^|[^A-Za-z0-9/+=])[A-Za-z0-9/+=]&#123;40&#125;($|[^A-Za-z0-9/+=])'</span> <span class="variable">$FILES</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$KEY_ID</span>"</span> ] || [ -n <span class="string">"<span class="variable">$KEY</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">exec</span> &lt; /dev/tty <span class="comment"># Capture input</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"=========== Possible AWS Access Key IDs ==========="</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;KEY_ID&#125;</span>"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"=========== Possible AWS Secret Access Keys ==========="</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;KEY&#125;</span>"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">read</span> -p <span class="string">"[AWS Key Check] Possible AWS keys found. Commit files anyway? (y/N) "</span> yn</span><br><span class="line">            <span class="keyword">if</span> [ <span class="string">"<span class="variable">$yn</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">                yn=<span class="string">'N'</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">case</span> <span class="variable">$yn</span> <span class="keyword">in</span></span><br><span class="line">                [Yy] ) <span class="built_in">exit</span> 0;;</span><br><span class="line">                [Nn] ) <span class="built_in">exit</span> 1;;</span><br><span class="line">                * ) <span class="built_in">echo</span> <span class="string">"Please answer y or n for yes or no."</span>;;</span><br><span class="line">            <span class="keyword">esac</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">        <span class="built_in">exec</span> &lt;&amp;- <span class="comment"># Release input</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normal exit</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></li></ul><h3 id="Prevent-commiting-invalid-json-files"><a href="#Prevent-commiting-invalid-json-files" class="headerlink" title="Prevent commiting invalid json files"></a>Prevent commiting invalid json files</h3><ul><li>file name : pre-commit.json<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">git_dir=$(git rev-parse --show-toplevel)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> $(git diff-index --name-only --diff-filter=ACM --cached HEAD -- \</span><br><span class="line">    | grep -E <span class="string">'\.((js)|(json))$'</span>); <span class="keyword">do</span></span><br><span class="line">    python -mjson.tool <span class="variable">$file</span> 2&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ] ; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">"Find unbroken json in <span class="variable">$git_dir</span>/<span class="variable">$file</span>, is that what you intended? [y|n] "</span> -n 1 -r &lt; /dev/tty</span><br><span class="line">        <span class="built_in">echo</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$REPLY</span> | grep -E <span class="string">'^[Yy]$'</span> &gt; /dev/null</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">exit</span> 0</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="Prevent-commiting-yaml-files-with-quote-in-key"><a href="#Prevent-commiting-yaml-files-with-quote-in-key" class="headerlink" title="Prevent commiting yaml files with quote in key"></a>Prevent commiting yaml files with quote in key</h3><p>Note that standard yaml allows containing quote in keys, but that is probably not something we intented to do.</p><ul><li><p>file name : pre-commit.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">git_dir=$(git rev-parse --show-toplevel)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> $(git diff-index --name-only --diff-filter=ACM --cached HEAD -- \</span><br><span class="line">    | grep -E <span class="string">'\.((yaml)|(yml))$'</span>); <span class="keyword">do</span></span><br><span class="line">    python3 <span class="string">"<span class="variable">$&#123;git_dir&#125;</span>/.git/hooks/is_yaml_key_contains_quote.py"</span> <span class="variable">$file</span></span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ] ; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">"Find quote in key in <span class="variable">$git_dir</span>/<span class="variable">$file</span>, is that what you intended? [y|n] "</span> -n 1 -r &lt; /dev/tty</span><br><span class="line">        <span class="built_in">echo</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$REPLY</span> | grep -E <span class="string">'^[Yy]$'</span> &gt; /dev/null</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">exit</span> 0</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li><li><p>file name : is_yaml_key_contains_quote.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_quote_in_key</span><span class="params">(json_obj)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> json_obj:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> type(json_obj) <span class="keyword">is</span> list:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_obj:</span><br><span class="line">            find_quote_in_key(item)</span><br><span class="line">    <span class="keyword">if</span> type(json_obj) <span class="keyword">is</span> dict:</span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> json_obj.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"\""</span> <span class="keyword">in</span> key <span class="keyword">or</span> <span class="string">"'"</span> <span class="keyword">in</span> key:</span><br><span class="line">                print(<span class="string">"quote is in : &#123;&#125;"</span>.format(key))</span><br><span class="line">                <span class="keyword">global</span> is_quote_found</span><br><span class="line">                is_quote_found = <span class="keyword">True</span></span><br><span class="line">            find_quote_in_key(value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">file_path = sys.argv[<span class="number">1</span>]</span><br><span class="line">is_quote_found = <span class="keyword">False</span></span><br><span class="line">yaml_content = <span class="string">""</span></span><br><span class="line"><span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> stream:</span><br><span class="line">    yaml_content = yaml.load(stream)</span><br><span class="line">json_obj = json.dumps(yaml_content)</span><br><span class="line">find_quote_in_key(json.loads(json_obj))</span><br><span class="line"><span class="keyword">if</span> is_quote_found:</span><br><span class="line">    exit(<span class="number">1</span>)</span><br><span class="line">exit(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Note : It is needed to install python3 and pyymal for running this python hook.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew install python3</span><br><span class="line">pip3 install pyyaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="Preventing-push-to-the-master-branch"><a href="#Preventing-push-to-the-master-branch" class="headerlink" title="Preventing push to the master branch"></a>Preventing push to the master branch</h3><p>(Script is <a href="https://github.com/smiley/git-hooks/blob/master/pre-push/prevent-push-to-protected-branch.sh" target="_blank" rel="noopener">from</a>)</p><ul><li>file name : pre-push.protect-master<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">protected_branch=<span class="string">'push-test'</span></span><br><span class="line">current_branch=$(git symbolic-ref HEAD | sed -e <span class="string">'s,.*/\(.*\),\1,'</span>)</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$protected_branch</span> = <span class="variable">$current_branch</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">read</span> -p <span class="string">"You're about to push <span class="variable">$protected_branch</span>, is that what you intended? [y|n] "</span> -n 1 -r &lt; /dev/tty</span><br><span class="line">    <span class="built_in">echo</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$REPLY</span> | grep -E <span class="string">'^[Yy]$'</span> &gt; /dev/null</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></li></ul><p>Don’t forget to execute : ln -s hook-chain pre-push.</p><h2 id="Install-hooks"><a href="#Install-hooks" class="headerlink" title="Install hooks"></a>Install hooks</h2><p>In any git project, run git init will install the hooks under .git/hooks/ folder, however, the existing files won’t be overriden, meaning it is needed to delete the existing hooks in the git repository after updating the hook scripts.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm .git/hooks/*</span><br><span class="line">git init</span><br></pre></td></tr></table></figure><p>I suggest creating an alias command for doing that.</p><p>Copy the following in the ~/.profile file</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> my_git_init=<span class="string">'rm .git/hooks/*; git init'</span></span><br></pre></td></tr></table></figure><p>Then run my_git_init under the git project will always delete the existing hooks and install the latest ones.</p>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop local mode and distributed mode</title>
      <link href="/2018/12/09/hadoop-local-and-distributed-mode/"/>
      <url>/2018/12/09/hadoop-local-and-distributed-mode/</url>
      
        <content type="html"><![CDATA[<p>Whether a job runs in local mode or distributed mode is decided by <em>mapreduce.framework.name</em>. In local mode, the mapper and reducer will run locally in the same JVM with the client. In distributed mode, the job will be submitted to the resource manager. This article aims at digging into the related source code.</p><h2 id="Flow-of-execution"><a href="#Flow-of-execution" class="headerlink" title="Flow of execution"></a>Flow of execution</h2><ul><li><p>Create JobClient</p><ul><li>Create <em>JobClient</em> which is primary interact with the cluster.</li><li><p>During JobClient initialization, create <em>Cluster</em> which provides access to the cluster.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.JobClient</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(JobConf conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">       ...    </span><br><span class="line">       cluster = <span class="keyword">new</span> Cluster(conf);</span><br><span class="line">       ...  </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>During Cluster initialization, use loaded ClientProtocalProvider list to create ClientProtocol until get one, ClientProtocal is the protocal for client and central jobtracker to communicate.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapreduce.Cluster</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">         ...</span><br><span class="line">    <span class="keyword">for</span> (ClientProtocolProvider provider : providerList) &#123;</span><br><span class="line">        ClientProtocol clientProtocol = <span class="keyword">null</span>;</span><br><span class="line">        clientProtocol = provider.create(..., conf);</span><br><span class="line">        <span class="keyword">if</span> (clientProtocol != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>There are two kinds of ClientProtocalProvider, one is LocalClientProtocolProvider for local, the other is YarnClientProtocolProvider for communicating with yarn.</p><ul><li><p>If <em>mapreduce.framework.name</em> is yarn, then create YarnRunner.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.YarnClientProtocolProvider</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ClientProtocol <span class="title">create</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (MRConfig.YARN_FRAMEWORK_NAME.equals(conf.get(MRConfig.FRAMEWORK_NAME))) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> YARNRunner(conf);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>If <em>mapreduce.framework.name</em> is local, then create LocalJobRunner, and set map to be 1.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalClientProtocolProvider</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ClientProtocol <span class="title">create</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    String framework =</span><br><span class="line">        conf.get(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);</span><br><span class="line">    <span class="keyword">if</span> (!MRConfig.LOCAL_FRAMEWORK_NAME.equals(framework)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    conf.setInt(JobContext.NUM_MAPS, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> LocalJobRunner(conf);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>Submit job through JobClient</p><ul><li><p>Get new JobId</p><ul><li><p>For local execution, generate local new jobId using LocalJobRunner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalJobRunner</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">synchronized</span> org.apache.hadoop.mapreduce.<span class="function">JobID <span class="title">getNewJobID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> org.apache.hadoop.mapreduce.JobID(<span class="string">"local"</span> + randid, ++jobid);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>For distributed execution, get new JobId from resource manager through YarnRunner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.YARNRunner</span><br><span class="line">  <span class="function"><span class="keyword">public</span> JobID <span class="title">getNewJobID</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> resMgrDelegate.getNewJobID();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Actual submit job</p><ul><li><p>For local execution</p><ul><li><p>Create a Job object. Job extends Thread, and can be run as a Thread.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalJobRunner</span><br><span class="line">   <span class="keyword">public</span> org.apache.hadoop.mapreduce.<span class="function">JobStatus <span class="title">submitJob</span><span class="params">(...)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      Job job = <span class="keyword">new</span> Job(JobID.downgrade(jobid), jobSubmitDir);</span><br><span class="line">      ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>At the end of the constructor of Job, it starts itself as a Thread.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalJobRunner</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Job</span><span class="params">(JobID jobid, String jobSubmitDir)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">this</span>.start();</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>For distributed execution, submit the job to resource manager.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.YARNRunner</span><br><span class="line">    public JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)&#123;</span><br><span class="line">        ...</span><br><span class="line">        ApplicationId applicationId =</span><br><span class="line">          resMgrDelegate.submitApplication(appContext);</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We can see that, whether a job runs in local mode or distributed mode is decided by <em>mapreduce.framework.name</em>. In local mode, the mapper and reducer will run locally in the same JVM with the client. In distributed mode, the job will be submitted to the resource manager.</p>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive configuration understanding</title>
      <link href="/2018/11/19/hive-configuration-best-practise/"/>
      <url>/2018/11/19/hive-configuration-best-practise/</url>
      
        <content type="html"><![CDATA[<p>This article aims at introducing what are the manually configured settings that override the default during using hive.</p><h2 id="environment"><a href="#environment" class="headerlink" title="environment"></a>environment</h2><p>This article is based on hive 2.3, hadoop 2.7, running hive on mapreduce.</p><h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>Hive and hadoop have many configurations and it is not striaightforward to know what configurations need to be override manually. This article aims at listing up all such configurations that we can special take care of.</p><h2 id="configuration-reference"><a href="#configuration-reference" class="headerlink" title="configuration reference"></a>configuration reference</h2><p>There are the links of the latest explanations of the configurations in the official wiki . </p><ul><li>hadoop<ul><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">core-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">hdfs-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="noopener">mapred-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a></li></ul></li><li>hive<ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties" target="_blank" rel="noopener">hive-site.xml</a></li></ul></li></ul><h2 id="overridden-configurations"><a href="#overridden-configurations" class="headerlink" title="overridden configurations"></a>overridden configurations</h2><p>These are the configurations that need to take special care and be configured manually to override the default value.</p><ul><li><p>Basic settings</p><ul><li>purpose<ul><li>There are some basic settings.</li></ul></li><li>configuration<ul><li>fs.defaultFS<ul><li>default : file:///</li><li>configured :  hdfs://&lt;namenode_ip&gt;:8020</li><li>explain : This configures the default file system. By default, it is local file system, change it to use distributed file system.</li></ul></li><li>mapreduce.framework.name<ul><li>default : local</li><li>configured : yarn</li><li>explain : The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.</li></ul></li><li>yarn.resourcemanager.hostname <ul><li>default:  0.0.0.0</li><li>configured: <the real="" ip=""></the></li><li>explain: The hostname of the RM.</li></ul></li></ul></li></ul></li><li><p>Enable yarn log aggregation</p><ul><li>purpose <ul><li>By default, yanr log aggregation is disabled, and yarn logs(container logs) are saved in the local of the instances where the containers run. In order to check the log, it is needed to first check the instance ip of a container, then login the instance and open the corresponding log file. When yarn log aggregation is enabled, the container’s log will be aggregated to some configured place(HDFS, s3, etc.), and it is able to check the yarn log through the yarn UI, which becomes much simpler.</li></ul></li><li>configuration <ul><li>yarn.log-aggregation-enable<ul><li>default : false</li><li>configured : true</li><li>explain : This enables the yarn log aggregation.</li></ul></li><li>yarn.nodemanager.remote-app-log-dir<ul><li>default : /tmp/logs for hdfs</li><li>configured : e.g. s3a://… for s3</li><li>explain : This specifies the prefix of the actual path of the aggregated logs. You may want to include the cluster name in the path.</li></ul></li><li>yarn.log-aggregation.retain-seconds<ul><li>default : -1</li><li>explain : This specifies how long the aggregated logs will be kept. The default value -1 indicates keeping the logs forever. Too big value may cause a waste of disk resource, too small may make you cannot find logs when investigate recent issues. 2 weeks or so may be helpful.</li></ul></li><li>yarn.nodemanager.log-aggregation.compression-type<ul><li>default: none</li><li>configured: gz</li><li>explain: If, and how, aggregated logs should be compressed.</li></ul></li><li>mapreduce.job.userlog.retain.hours<ul><li>default : 24</li><li>explain : This specifies the maximum time in hours that the local yarn logs will be retained after job completion. You may want to adjust this setting together.</li></ul></li><li>mapreduce.jobhistory.max-age-ms<ul><li>default : 604800000</li><li>explain : Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week). We can make it 2 weeks.</li></ul></li></ul></li><li><a href="https://renenyffenegger.ch/notes/development/Apache/Hadoop/Ycccc-=ARN/log-aggregation" target="_blank" rel="noopener">helpful links</a></li></ul></li><li>Clean HDFS trash regularly<ul><li>purpose<ul><li>By default, the trash folder is not cleaned up automatically. We can configure to make it be cleaned up regularly to reduce disk cost.</li></ul></li><li>configuration<ul><li>fs.trash.interval    <ul><li>default : 0</li><li>configured : 360</li><li>explain : will be deleted after 6 hours</li></ul></li></ul></li></ul></li><li>Enable hive client authorization<ul><li>purpose<ul><li>By default, hive client authorization is disabled. Enabling hive client authorization can help prevent users from doing operations they are not supposed to do.</li></ul></li><li>configuration<ul><li>hive.security.authorization.enabled<ul><li>default : false</li><li>configured : true</li><li>explain : Enables the hive client authorization.</li></ul></li><li>There are also other settings as needed. I will leave it as TODO.</li></ul></li><li>helpful links<ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization" target="_blank" rel="noopener">SQL Standard Based Hive Authorization</a></li></ul></li></ul></li><li>Enable HDFS compression<ul><li>configuration<ul><li>io.compression.codecs<ul><li>default : </li><li>configured :  org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec</li><li>explain : A comma-separated list of the compression codec classes that can be used for compression/decompression which precedence of others loaded by from the classpath.</li></ul></li><li>mapreduce.map.output.compress<ul><li>default : false</li><li>configured : true</li><li>explain : Compress the output of map before sending accross the network.</li></ul></li><li>mapreduce.map.output.compress.codec<ul><li>default : org.apache.hadoop.io.compress.DefaultCodec</li><li>configured : org.apache.hadoop.io.compress.SnappyCodec or org.apache.hadoop.io.compress.GzipCodec</li><li>explain : If the map outputs are compressed, how should they be compressed?</li></ul></li><li>mapreduce.output.fileoutputformat.compress.type<ul><li>default : RECORD</li><li>configured : BLOCK</li><li>explain : If the job outputs are to compressed as SequenceFiles, how should they be compressed?</li></ul></li></ul></li></ul></li><li>Fail jobs when exceed disk usage<ul><li>configuration<ul><li>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<ul><li>default : true</li><li>configured : false</li></ul></li></ul></li></ul></li><li>dfs related<ul><li>configuration<ul><li>dfs.namenode.avoid.read.stale.datanode<ul><li>default : false</li><li>configured : true</li></ul></li><li>dfs.namenode.avoid.write.stale.datanode<ul><li>default : false</li><li>configured : true</li></ul></li></ul></li></ul></li><li>save job result in s3(option)<ul><li>configuration<ul><li>mapreduce.jobhistory.done-dir <ul><li>default: ${yarn.app.mapreduce.am.staging-dir}/history/done </li><li>configured: s3 path</li></ul></li></ul></li></ul></li><li>speculative configs<ul><li>configuration<ul><li>mapreduce.map.speculative <ul><li>default: true</li><li>configured: false</li></ul></li><li>mapreduce.reduce.speculative<ul><li>default: true</li><li>configured: false</li></ul></li></ul></li></ul></li><li>enable namenode restart<ul><li>configuration<ul><li>yarn.nodemanager.recovery.enabled<ul><li>default: false</li><li>configured: true</li></ul></li><li>yarn.nodemanager.address<ul><li>default:  ${yarn.nodemanager.hostname}:0</li><li>configured: 0.0.0.0:45454</li><li>explain: Default value makes namenode use different port before and after a restart, and the running clients will be broken after the restart, explicitly setting the port can solve the issue.</li></ul></li></ul></li><li><a href="https://hadoop.apache.org/docs/r2.8.0/hadoop-yarn/hadoop-yarn-site/NodeManager.html" target="_blank" rel="noopener">helpful links</a></li></ul></li><li>enable resource manager restart<ul><li>configuration<ul><li>yarn.resourcemanager.recovery.enabled<ul><li>default: false</li><li>configured: true</li></ul></li></ul></li><li><a href="https://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">helpful link</a></li></ul></li><li>hive configs<ul><li>configuration<ul><li>hive.auto.convert.join<ul><li>configured: false</li></ul></li><li>hive.auto.convert.join.noconditionaltask<ul><li>configured: false</li></ul></li><li>hive.exec.compress.intermediate<ul><li>configured: true</li></ul></li><li>hive.exec.parallel<ul><li>configured: true</li></ul></li><li>hive.fetch.task.conversion<ul><li>default: more</li><li>configured: none</li></ul></li><li>hive.groupby.orderby.position.alias<ul><li>configured: true</li></ul></li><li>hive.log.explain.output<ul><li>configured: true</li></ul></li><li>hive.mapred.reduce.tasks.speculative.execution<ul><li>configured: false</li></ul></li><li>hive.optimize.reducededuplication<ul><li>configured: false</li></ul></li><li>hive.resultset.use.unique.column.names<ul><li>configured: false</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive scratch directory</title>
      <link href="/2018/11/17/hive-scratch-working-directory/"/>
      <url>/2018/11/17/hive-scratch-working-directory/</url>
      
        <content type="html"><![CDATA[<p>This article aims at explaining hive scratch directory.</p><h2 id="Scratch-directory-usage"><a href="#Scratch-directory-usage" class="headerlink" title="Scratch directory usage"></a>Scratch directory usage</h2><p>Hive scratch directory is a temporary working space for storing the plans for different map/reduce stages of the query as well as the intermediate outputs of these stages.</p><h2 id="Scratch-directory-clean-up"><a href="#Scratch-directory-clean-up" class="headerlink" title="Scratch directory clean up"></a>Scratch directory clean up</h2><p>Hive scratch directory is usually cleaned up by the hive client when the query finishes. However, some data may be left behind if hive client terminates abnormally. Hive server2 contains a thread (<a href="https://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/session/ClearDanglingScratchDir.java#L43-L55" target="_blank" rel="noopener">ClearDanglingScratchDir</a>) to clean up the remaining files, we can also write our own script to do the clean up if not running Hive server2.</p><h2 id="Scratch-directory-types"><a href="#Scratch-directory-types" class="headerlink" title="Scratch directory types"></a>Scratch directory types</h2><p>Hive queries may be procesed in local(the instance which hive client is invoked) or in remote(hadoop cluster). There also have two kinds of scratch dir accordingly, one in local, the other in hdfs.</p><h2 id="Scratch-directory-configuration"><a href="#Scratch-directory-configuration" class="headerlink" title="Scratch directory configuration"></a>Scratch directory configuration</h2><p><em>hive.exec.local.scratchdir</em> for local and <em>hive.exec.scratchdir</em> for HDFS(<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">hive configuration</a>).</p><p>Note that since hive 0.14.0, the HDFS scratch directory created will be <em>${hive.exec.scratchdir}\${user_name}</em> indicating it supports multi-tenant natively and there is no need to include user_id in the value.</p><h2 id="Scratch-directory-example"><a href="#Scratch-directory-example" class="headerlink" title="Scratch directory example"></a>Scratch directory example</h2><p>We run a simple query and see what are the files generated in the scratch directory.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--@INTERNAL hive_version:hive2</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> col1&gt;<span class="number">0</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> col2</span><br></pre></td></tr></table></figure></p><ul><li><p>when query is submitted to the cluster and waiting for containers</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br></pre></td></tr></table></figure><ul><li>-ext- : a dir indicates the final query output</li><li>-mr- : a output directory for each MapReduce job</li><li>map.xml : map plan</li><li>reduce.xml : reduce plan</li></ul></li><li><p>when query is running in the hadoop cluster</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001/000000_0</span><br></pre></td></tr></table></figure><ul><li>data is generated in the -ext- dir  </li></ul></li><li>when query finished, scratch dir with all files are cleaned up</li></ul><h2 id="Scratch-directory-related-INFO-logs"><a href="#Scratch-directory-related-INFO-logs" class="headerlink" title="Scratch directory related INFO logs"></a>Scratch directory related INFO logs</h2><p>These info logs are generated when running the query in the previous section.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/_tmp_space.db</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020$/&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br><span class="line">exec.FileSinkOperator: Moving tmp dir: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001 to: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001</span><br></pre></td></tr></table></figure><ul><li><p>First several HDFS scratch directories are created during start <a href="(https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L698-L704">SessionState</a>.</p></li><li><p>_hive.hdfs.session.path = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}</p></li><li>hive.exec.plan = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}/${context execution id}-${task runner id}/-mr-${path id}/${random uuid}<ul><li><a href="https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L414" target="_blank" rel="noopener">hive.session.id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L313" target="_blank" rel="noopener">context execution id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L491" target="_blank" rel="noopener">task running id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L685" target="_blank" rel="noopener">path id</a></li><li><a href="https://github.com/apache/hive/blob/6d713b6564ecb9d1ae0db66c3742d2a8bc347211/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L668" target="_blank" rel="noopener">random uuid</a></li></ul></li><li>map.xml path = ${hive.exec.plan}/map.xml</li><li>reduce.xml path = ${hive.exec.plan}/reduce.xml</li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>How to fast fail hive jobs</title>
      <link href="/2018/11/15/hive-job-fast-fail/"/>
      <url>/2018/11/15/hive-job-fast-fail/</url>
      
        <content type="html"><![CDATA[<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>When running hive jobs in hadoop clusters on mapreduce, we always set the limitation of how much local and hdfs disk a job can use at most. Such limitation is per job basis and it can prevent a single job from using up too much disk resource and causing a node or a cluster unstable.</p><p>During running one job, if any task belong to the job finds the limitation is exceeded, the task will fail and at such time, we want to <strong>fast fail the job instead of retrying the failed tasks many times</strong>. Because since the local or hdfs max disk usage limitation is reached, the retry will probably still fail and is not much helpful.</p><h2 id="appoaches"><a href="#appoaches" class="headerlink" title="appoaches"></a>appoaches</h2><p>I will introduce how to configure to fast fail hive jobs when the local or hdfs limitation is exceeded.</p><h3 id="fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node"><a href="#fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node" class="headerlink" title="fast fail a job when too much local disk is used in one node"></a>fast fail a job when too much <em>local disk</em> is used in one node</h3><ul><li><p><strong>hadoop configuration</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.bytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>322122547200<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>implementation detail</strong><br>A task(org.apache.hadoop.mapred.Task) contains a thread checking all local working directories(mapreduce.cluster.local.dir), and will fast fail the job if any local working directory exceeds the limitation.</p></li><li><p><strong>required hadoop version</strong><br>3.1.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7022" target="_blank" rel="noopener">MAPREDUCE-7022</a></p></li></ul><h3 id="fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster"><a href="#fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster" class="headerlink" title="fast fail a job when too much hdfs disk is used in the cluster"></a>fast fail a job when too much <em>hdfs disk</em> is used in the cluster</h3><ul><li><p><strong>configuration</strong></p><ul><li><strong>hive configuration</strong><br>Set hive config hive.exec.scratchdir to per job basis, and set the quota limitation for the path.</li><li><strong>hadoop configuration</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>internal implementation</strong><br>A subclass of ClusterStorageCapacityExceededException will be thrown if the cluster capacity limitation is exceeded and then YarnChild will fast fail the job.</p></li><li><p><strong>required hadoop version</strong><br>3.3.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7148" target="_blank" rel="noopener">MAPREDUCE-7148</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
