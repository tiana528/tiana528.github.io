<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>build hadoop trunck branch and import into eclipse in mac</title>
      <link href="/2019/12/03/build-hadoop-trunck-branch-and-import-into-eclipse/"/>
      <url>/2019/12/03/build-hadoop-trunck-branch-and-import-into-eclipse/</url>
      
        <content type="html"><![CDATA[<h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>I tried to build the trunck branch of hadoop git repository and imported it into eclipse then resolved the issues.<br>The date I did it is 2019/11/03, the latest commit is : 6b2d6d4aafb110bef1b77d4ccbba4350e624b57d.</p><h1 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h1><h2 id="download-hadoop-repository"><a href="#download-hadoop-repository" class="headerlink" title="download hadoop repository"></a>download hadoop repository</h2><ul><li>git clone <a href="https://github.com/apache/hadoop" target="_blank" rel="noopener">https://github.com/apache/hadoop</a></li><li>cd hadoop</li><li>cat hadoop-project/pom.xml | grep proto<ul><li>check the version of the protoc that is being used, in my case, version 3.5.1 is being used</li></ul></li></ul><h2 id="compile-and-build-proto"><a href="#compile-and-build-proto" class="headerlink" title="compile and build proto"></a>compile and build proto</h2><p>Note that the same version is recommended to be used as the one we checked in the previous step.</p><p>I am installing it under <code>/usr/local/protoc</code>, you can install whereever you want, and even someplace in your home folder.</p><ul><li>git clone <a href="https://github.com/protocolbuffers/protobuf.git" target="_blank" rel="noopener">https://github.com/protocolbuffers/protobuf.git</a></li><li>cd protobuf</li><li>git checkout tags/v3.5.1 -b v3.5.1</li><li>autoreconf –install</li><li>./configure –prefix=/usr/local/protoc</li><li>make </li><li>sudo make install</li><li>Update ~/.bashrc file to add a line of : PATH=”/usr/local/protoc/bin:${PATH}”</li><li>source ~/.bashrc</li></ul><h2 id="compile-hadoop"><a href="#compile-hadoop" class="headerlink" title="compile hadoop"></a>compile hadoop</h2><ul><li>cd hadoop</li><li>mvn clean install -DskipTests=true</li><li>mvn eclipse:clean eclipse:eclipse -DskipTests</li></ul><h2 id="import-hadoop-into-eclipse-and-solve-issues"><a href="#import-hadoop-into-eclipse-and-solve-issues" class="headerlink" title="import hadoop into eclipse and solve issues"></a>import hadoop into eclipse and solve issues</h2><p>After importing all hadoop projects into eclipse, there are 400 build errors shown in the error console. I will show how to solve them manually.</p><h3 id="about-cannot-nest-issue"><a href="#about-cannot-nest-issue" class="headerlink" title="about cannot nest issue"></a>about cannot nest issue</h3><p>(1) error message : hadoop-yarn-common project has build error, Cannot nest ‘hadoop-yarn-common/src/test/resources/resource-types’ inside ‘hadoop-yarn-common/src/test/resources’. To enable the nesting exclude ‘resource-types/‘ from ‘hadoop-yarn-common/src/test/resources’</p><p>(2) solve approach : hadoop-yarn-common project -&gt; properties -&gt; build path -&gt; add <code>resource-types/</code> to the exclude path</p><p>(3) remaining errors : 399</p><h3 id="about-missing-required-folder-issue"><a href="#about-missing-required-folder-issue" class="headerlink" title="about missing required folder issue"></a>about missing required folder issue</h3><p>(1) error message : Project ‘hadoop-streaming’ is missing required source folder: ‘/Users/wang.yan/public_work/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/conf’</p><p>(2) solve appraoch : hadoop-streaming -&gt; properties -&gt; source -&gt; remove the one has error</p><p>(3) remaining errors: 550 (the number increases lol)</p><h3 id="about-package-info-issue"><a href="#about-package-info-issue" class="headerlink" title="about package-info issue"></a>about package-info issue</h3><p>(1) error message : The type package-info is already defined    package-info.java    /hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants</p><p>(2) solve approach : hadoop-azure -&gt; build -&gt; resource -&gt; src/main/java exclude : <code>**/package-info.java</code></p><p>(3) remaining errors : 545</p><h3 id="about-Builder-cannot-be-resolved-issue"><a href="#about-Builder-cannot-be-resolved-issue" class="headerlink" title="about Builder cannot be resolved issue"></a>about Builder cannot be resolved issue</h3><p>(1) error message : Builder cannot be resolved to a type    ITUseMiniCluster.java    /hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example</p><p>(2) solve approach :  hadoop-client-integration-tests -&gt; add project dependency of hadoop-hdfs, hadoop-common, hadoop-hdfs-client</p><p>(3) remaining errors : 516</p><h3 id="about-access-restriction-issue"><a href="#about-access-restriction-issue" class="headerlink" title="about access restriction issue"></a>about access restriction issue</h3><p>(1) error message : The method ‘Unsafe.arrayBaseOffset(Class&lt;?&gt;)’ is not API (restriction on required library ‘/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home/jre/lib/rt.jar’)    FastByteComparisons.java    /hadoop-common/src/main/java/org/apache/hadoop/io</p><p>(2) solve approach : Window -&gt; Preferences -&gt; Java -&gt; Compiler -&gt; Errors/Warnings -&gt; Deprecated and restricted API -&gt; Forbidden reference (access rules) -&gt; Warnings</p><p>(3) remaining errors : 429</p><h3 id="about-AvroRecord-issue"><a href="#about-AvroRecord-issue" class="headerlink" title="about AvroRecord issue"></a>about AvroRecord issue</h3><p>(1) error message :  AvroRecord cannot be resolved to a type    TestAvroSerialization.java    /hadoop-common/src/test/java/org/apache/hadoop/io/serializer/avro</p><p>(2) solve approach : </p><ul><li>Download avro-tools-x.x.x.jar jar file from the Internet</li><li>cd hadoop-common-project/hadoop-common/src/test/avro</li><li>java -jar ~/Downloads/avro-tools-1.7.7.jar compile schema avroRecord.avsc ../java</li></ul><p>(3) remaining errors : 412</p><h3 id="about-Proto"><a href="#about-Proto" class="headerlink" title="about Proto"></a>about Proto</h3><p>(1) error message : EchoRequestProto cannot be resolved    RPCCallBenchmark.java    /hadoop-common/src/test/java/org/apache/hadoop/ipc    line 386    Java Problem</p><p>(2) solve approach :</p><ul><li>cd hadoop-common-project\hadoop-common\src\test\proto</li><li>protoc –java_out=../java *.proto</li></ul><p>(3) remaining errors : 137</p><h3 id="more-about-proto"><a href="#more-about-proto" class="headerlink" title="more about proto"></a>more about proto</h3><p>Do the same as above</p><ul><li>cd ./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/proto/</li><li><p>protoc –java_out=../java *.proto –proto_path=.  –proto_path=/Users/wang.yan/hadoop_source/hadoop/./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/proto/ –proto_path=/Users/wang.yan/hadoop_source/hadoop/hadoop-common-project/hadoop-common/src/main/proto</p></li><li><p>cd ./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/proto/test_client_tokens.proto</p></li><li><p>protoc –java_out=../java *.proto –proto_path=.  –proto_path=/Users/wang.yan/hadoop_source/hadoop/./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/proto/ –proto_path=/Users/wang.yan/hadoop_source/hadoop/hadoop-common-project/hadoop-common/src/main/proto</p></li><li><p>cd ./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/proto/</p></li><li>protoc –java_out=../java *.proto –proto_path=.  –proto_path=/Users/wang.yan/hadoop_source/hadoop/./hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/proto/ –proto_path=/Users/wang.yan/hadoop_source/hadoop/hadoop-common-project/hadoop-common/src/main/proto</li></ul><p>Remaining errors : 0</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>how oracle java license change affect users</title>
      <link href="/2019/09/23/how-oracle-java-license-change-affect-users/"/>
      <url>/2019/09/23/how-oracle-java-license-change-affect-users/</url>
      
        <content type="html"><![CDATA[<h1 id="Oracle-java"><a href="#Oracle-java" class="headerlink" title="Oracle java"></a>Oracle java</h1><h2 id="platforms"><a href="#platforms" class="headerlink" title="platforms"></a>platforms</h2><p>oracle java has 3 platforms, Java SE for standard usage, Java EE for enterprise usage, and Java ME for micro usage.<br>They are all specifications and are not implementations.</p><p>Some specification examples:</p><ul><li>Java SE<ul><li>specification<ul><li><a href="https://docs.oracle.com/javase/specs/" target="_blank" rel="noopener">https://docs.oracle.com/javase/specs/</a></li><li>Java Language Specification and Java Virtual Machine Specification<ul><li>Grammars, Lexical Structure, Conversions, Classes, Interfaces, Arrays, Threads…</li></ul></li><li>Java Virtual Machine Specification<ul><li>Compiler, loading, linking, instruction set…</li></ul></li></ul></li><li>api <ul><li><a href="https://docs.oracle.com/javase/8/docs/api/" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/api/</a></li><li>java.applet, java.io, java.lang, java.sql, java.util…</li></ul></li></ul></li><li>Java EE<ul><li><a href="https://javaee.github.io/javaee-spec/javadocs/" target="_blank" rel="noopener">https://javaee.github.io/javaee-spec/javadocs/</a></li><li>javax.annotation, javax.xml.rpc.handler…</li></ul></li><li>Java ME<ul><li><a href="https://www.oracle.com/java/technologies/javameoverview.html" target="_blank" rel="noopener">https://www.oracle.com/java/technologies/javameoverview.html</a></li><li>IOT…</li></ul></li></ul><h2 id="relationship-between-jdk-jre-and-Java-SE"><a href="#relationship-between-jdk-jre-and-Java-SE" class="headerlink" title="relationship between jdk, jre and Java SE"></a>relationship between jdk, jre and Java SE</h2><ul><li>Refer to <a href="https://docs.oracle.com/javase/8/docs/" target="_blank" rel="noopener">https://docs.oracle.com/javase/8/docs/</a>, Java SE defines the specification. JDK and JRE are implementations of the Java SE.</li><li>JDK is a superset of JRE. Meaning when you download JDK, the JRE is always included.</li><li>JDK is for developing programs, JRE is for running programs.</li></ul><h2 id="oracle-jdk-user-types"><a href="#oracle-jdk-user-types" class="headerlink" title="oracle jdk user types"></a>oracle jdk user types</h2><p>According to <a href="https://www.oracle.com/technetwork/java/javase/overview/oracle-jdk-faqs.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/overview/oracle-jdk-faqs.html</a>, there are at least three kinds of users.</p><ul><li>personal users<ul><li>Personal use is using Java on a desktop or laptop computer to do things such as to play games or run other personal applications. If you are using Java on a desktop or laptop computer as part of any business operations, that is not personal use.</li></ul></li><li>commercial users</li><li>Oracle Product users</li></ul><h2 id="oracle-jdk-license-changes-at-April-16-2019"><a href="#oracle-jdk-license-changes-at-April-16-2019" class="headerlink" title="oracle jdk license changes at April 16, 2019"></a>oracle jdk license changes at April 16, 2019</h2><p>Oracle JAVA SE product releases(include Oracle JDK/JRE) before April 16 2019 are BCL license, and are OTN license after April 16, 2019.<br>Oracle OpenJDK releases are still GPL license.</p><p>(According to the release note, <a href="https://www.java.com/en/download/faq/release_dates.xml" target="_blank" rel="noopener">https://www.java.com/en/download/faq/release_dates.xml</a>, Oracle JDK8 until jdk1.8-201 belong to BCL license, Oracle JDK8 later than it belong to OTN license.)<br>(Commercial users can use BCL license at no cost for most of the features, but commercial users cannot use OTN license at no cost.)</p><h2 id="do-you-need-to-pay-to-use-Oracle-JAVA"><a href="#do-you-need-to-pay-to-use-Oracle-JAVA" class="headerlink" title="do you need to pay to use Oracle JAVA?"></a>do you need to pay to use Oracle JAVA?</h2><ul><li>no cost<ul><li>All types of users can use <code>Oracle OpenJDK</code> at no cost.</li><li>Personal user and Oracle Product users can use <code>Oracle Java SE product</code>(Oracle JDK version before April 16, 2019) at no cost.</li></ul></li><li>need to pay<ul><li>Commercial users who use <code>Oracle Java SE product</code>(Oracle JDK version after April 16, 2019) will require a Java SE Subscription, meaning need to pay money to Oracle.</li></ul></li></ul><p>It means that if you use Oracle JDK8 in the commercial environments at no cost, the latest Oracle JDK8 version you can use is jdk1.8-201.<br>If you want to use newer versions(you would need to do it, for security patches), you need to either use Oracle OpenJDK or pay for the Oracle JDK.<br>If you want to pay for Oracle JDK, oracle says the price is <code>as low as $2.50/desktop user/month</code>, but I think this price is not low…</p><h2 id="suggestion-for-commercial-users-to-choose-which-JDK-to-use"><a href="#suggestion-for-commercial-users-to-choose-which-JDK-to-use" class="headerlink" title="suggestion for commercial users to choose which JDK to use"></a>suggestion for commercial users to choose which JDK to use</h2><p>Use Oracle OpenJDK for most of the cases.<br>Use and pay for Oracle JDK if you really need strong support from Oracle(like financial companies?), which I think is not the case for most of the companies.<br>There are also free JDKs that are not provided by Oracle, such as Zulu, AdoptOpenJDK, Corretto(from Amazon) and so on. They can also be alternative choices.</p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>how to pass AWS Solutions Architect Associate Exam</title>
      <link href="/2019/09/22/how-to-pass-AWS-Solutions-Architect-Associate-Exam/"/>
      <url>/2019/09/22/how-to-pass-AWS-Solutions-Architect-Associate-Exam/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>I passed the AWS solutions architect assciate exam today, and I would like to share some knowledge for people who want to prepare for the examination.<br>There are already a lot of other blogs talking about how to prepare for the exmaination. Actually there are too much information, and it is difficult to know <code>what is important</code> and <code>what is not important</code>.<br>Most blogs are only concentrating what are the important parts, and I will also tell you <strong>what is not important, so that you can save a lot of time</strong>.</p><p>This article is mainly for people who don’t have much experience with AWS(for example, you have no idea what is Cloudfront…), and want to know how to prepare for the exam efficiently.</p><h1 id="Description-of-the-examination"><a href="#Description-of-the-examination" class="headerlink" title="Description of the examination"></a>Description of the examination</h1><p>65 questions.<br>130 minutes.<br>I finished the exam within 50 minutes, I think you don’t need to worry about time.</p><h1 id="what-is-important-and-what-is-not-important"><a href="#what-is-important-and-what-is-not-important" class="headerlink" title="what is important and what is not important"></a>what is important and what is not important</h1><ul><li>Each question is very short, most questions are only 2 or 3 lines. Meaning basicly, you don’t need to do a lot of exercises which are very long.</li><li>Nearly no question requires you remember the actual number(except for the very very basic one). So you don’t need to remember things such as the limitation of the number of subnets in one vpc is 200.</li><li>I have seen more than half of the questions from the exercises I have done. So I think doing exercises can be very helpful for passing the exam.</li><li>There are many questions of AWS solutions associate you can find in the Internet, but most of them contain wrong results. I recommend choosing the places where people can discuss, so even if the answer is wrong, some people will discuss what the correct answer is.</li><li>The service I meet in the questions are all very common service, EC2, cloudfront, api gateway, vpc, elb, s3, rds, sts, storage gateway, sqs, beanstalk, route53, swf, dynamodb, CloudFormation, Snowball, kinesis, opswork, iam, lambda, redshift, ecs, redis, codedeploy, Cognito, elasticache, Aurora, CloudTrail, sns, directory service. These are important topics.</li><li>I haven’t seen any questions about devpay, FPS, budget, AppSync, Step Functions, X-Ray, Glue, Systems Manager, Workspace, organizations. So I think these are not important topics.</li></ul><h1 id="how-I-prepared-for-the-exam"><a href="#how-I-prepared-for-the-exam" class="headerlink" title="how I prepared for the exam"></a>how I prepared for the exam</h1><ul><li>read a book(500+ pages) : AWS Certified Solutions Architect Official Study Guide: Associate Exam<ul><li><a href="https://www.pdfdrive.com/aws-certified-solutions-architect-official-study-guide-associate-exam-d38558089.html" target="_blank" rel="noopener">https://www.pdfdrive.com/aws-certified-solutions-architect-official-study-guide-associate-exam-d38558089.html</a></li></ul></li><li>exam practise<ul><li>Udemy(need to pay, but not expensive)<ul><li>AWS Certified Solutions Architect Associate Practice Exams</li></ul></li><li>examtopics<ul><li>314 questions, people can leave comments</li></ul></li><li>briefmenow<ul><li><a href="https://www.briefmenow.org/amazon/what-additional-step-is-required-to-allow-access-from-t/" target="_blank" rel="noopener">https://www.briefmenow.org/amazon/what-additional-step-is-required-to-allow-access-from-t/</a></li><li>403 questions, people can leave comments</li></ul></li><li>also some other questions that I can find on the Internet, but the above ones are the most useful ones<ul><li><a href="https://d1.awsstatic.com/training-and-certification/docs/AWS_Certified_Solutions_Architect_Associate_Sample_Questions.pdf" target="_blank" rel="noopener">https://d1.awsstatic.com/training-and-certification/docs/AWS_Certified_Solutions_Architect_Associate_Sample_Questions.pdf</a></li><li><a href="https://www.whizlabs.com/learn/course/aws-csaa-practice-tests/quiz/14715" target="_blank" rel="noopener">https://www.whizlabs.com/learn/course/aws-csaa-practice-tests/quiz/14715</a></li><li><a href="https://awspro.academy/aws-certified-solutions-architect-associate-exam-free-20-questions/" target="_blank" rel="noopener">https://awspro.academy/aws-certified-solutions-architect-associate-exam-free-20-questions/</a></li><li><a href="https://www.awscoach.net/architect-associate-questions/" target="_blank" rel="noopener">https://www.awscoach.net/architect-associate-questions/</a></li><li><a href="https://www.awslagi.com/" target="_blank" rel="noopener">https://www.awslagi.com/</a></li><li><a href="https://pdf.certleader.com/AWS-Solution-Architect-Associate.pdf" target="_blank" rel="noopener">https://pdf.certleader.com/AWS-Solution-Architect-Associate.pdf</a></li><li><a href="https://chercher.tech/aws-certification/aws-certification-practice" target="_blank" rel="noopener">https://chercher.tech/aws-certification/aws-certification-practice</a></li><li><a href="https://quizlet.com/144321056/aws-certified-solutions-architect-associate-practice-questions-flash-cards/" target="_blank" rel="noopener">https://quizlet.com/144321056/aws-certified-solutions-architect-associate-practice-questions-flash-cards/</a></li><li><a href="https://www.proprofs.com/quiz-school/story.php?title=mtk3mjk2ng9i1t" target="_blank" rel="noopener">https://www.proprofs.com/quiz-school/story.php?title=mtk3mjk2ng9i1t</a></li><li><a href="https://vceguide.com/which-of-the-following-options-would-enable-an-equivalent-experience-for-users-on-both-continents/" target="_blank" rel="noopener">https://vceguide.com/which-of-the-following-options-would-enable-an-equivalent-experience-for-users-on-both-continents/</a></li><li><a href="https://www.briefmenow.org/amazon/aws-certified-solutions-architect-associate-2018-solutions-architect-is-designing-an-application-that-will-encrypt-all-data-in-an-amazon-redshift/" target="_blank" rel="noopener">https://www.briefmenow.org/amazon/aws-certified-solutions-architect-associate-2018-solutions-architect-is-designing-an-application-that-will-encrypt-all-data-in-an-amazon-redshift/</a></li><li>One more again, a lot of the answers are wrong. <strong> I recommend you find somewhere people can discuss or there is detailed explanation</strong>.</li></ul></li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>resilient architecture should also consider hard shutdown</title>
      <link href="/2019/04/14/resilient-architecture-should-also-consider-hard-shutdown/"/>
      <url>/2019/04/14/resilient-architecture-should-also-consider-hard-shutdown/</url>
      
        <content type="html"><![CDATA[<h1 id="graceful-shutdown-and-hard-shutdown"><a href="#graceful-shutdown-and-hard-shutdown" class="headerlink" title="graceful shutdown and hard shutdown"></a>graceful shutdown and hard shutdown</h1><p>When a program/service is shutdown unexpectedly, it may be graceful shutdown or hard shutdown.<br>When a service is killed with signal 15, if the service catches the signal and perform some cleanup before exit, it is graceful shutdown. If the service does not catch the signal and exits intermediately, it is hard shutdown.<br>When a service is killed with 9, the service cannot catch the signal and will be termidated immediately, this is also hard shutdown.</p><h1 id="hard-shutdown-is-unavoided"><a href="#hard-shutdown-is-unavoided" class="headerlink" title="hard shutdown is unavoided"></a>hard shutdown is unavoided</h1><p>You cannot prevent your service from hard shutdown, it may happen due to out of memory, disk full, operating system kernal crash, or even instance power off.</p><h1 id="how-to-create-resilient-architecture"><a href="#how-to-create-resilient-architecture" class="headerlink" title="how to create resilient architecture"></a>how to create resilient architecture</h1><p>Your service should be strong enough(still keep consistency, be able to recover) no matter the service is killed by 9 or by 15.<br>It means that you need to make the service handle graceful shutdown, which is quite straightforward, just adding a shutdown hook to do some cleanups.<br>It is not easy to make the service support the case of hard shutdown, many application side logic is needed here and there to support it.<br><strong>But it is very important to handle the case of hard shutdown to achieve high resilient architecture. If your data/service will be inconsistent and there is no way to recover whenever hard shutdown happens, it might just be unacceptable.</strong></p><h1 id="some-examples-of-high-resilient-architecture-which-support-hard-shutdown"><a href="#some-examples-of-high-resilient-architecture-which-support-hard-shutdown" class="headerlink" title="some examples of high resilient architecture which support hard shutdown"></a>some examples of high resilient architecture which support hard shutdown</h1><ul><li>One good example is mysql, it writes operation logs, and if the mysql process hard shutdown happens, it can still recover from the failure, and won’t have any inconsistency.</li><li>Another example is hadoop, a hadoop cluster may have thousands of nodes, and it can still work properly even if any one node is lost, the data is already duplicated on other nodes, the failed tasks will be retried on other nodes.</li><li>Windows is also a good example, if you shutdown the power of your windows PC, your windows can still work properly after you restart it.</li></ul>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>YARN UI detailed explanation</title>
      <link href="/2019/02/11/YARN-UI-detailed-explanation/"/>
      <url>/2019/02/11/YARN-UI-detailed-explanation/</url>
      
        <content type="html"><![CDATA[<h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>YARN UI url is {hadoop_master_ip:8088}. There is a lot of information in this UI as follows which is very helpful for analyzing issues. This article aims at explaning everything in detail.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Cluster</span><br><span class="line">    About</span><br><span class="line">    Nodes</span><br><span class="line">    Node Labels</span><br><span class="line">    Applications</span><br><span class="line">        NEW</span><br><span class="line">        NEW SAVING</span><br><span class="line">        SUBMITTED</span><br><span class="line">        ACCEPTED</span><br><span class="line">        RUNNING</span><br><span class="line">        FINISHED</span><br><span class="line">        FAILED</span><br><span class="line">        KILLED</span><br><span class="line">    SCHEDULER</span><br><span class="line">TOOLS</span><br><span class="line">    CONFIGURATION</span><br><span class="line">    LOCAL LOGS</span><br><span class="line">    SERVER STACKS</span><br><span class="line">    SERVER METRICS</span><br></pre></td></tr></table></figure></p><p>hadoop version is 2.7.3.</p><h1 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h1><h2 id="Common-Header"><a href="#Common-Header" class="headerlink" title="Common Header"></a>Common Header</h2><ul><li><p>Cluster Metrics</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Apps Submitted : 3220</span><br><span class="line">Apps Pending : 1</span><br><span class="line">Apps Running : 3</span><br><span class="line">Apps Completed : 3216</span><br><span class="line">Containers Running : 3</span><br><span class="line">Memory Used : 3 GB</span><br><span class="line">Memory Total : 9 GB</span><br><span class="line">Memory Reserved : 0 B</span><br><span class="line">VCores Used : 3</span><br><span class="line">VCores Total : 9</span><br><span class="line">VCores Reserved : 0</span><br><span class="line">Active Nodes : 3</span><br><span class="line">Decommissioned Nodes : 0</span><br><span class="line">Lost Nodes : 0</span><br><span class="line">Unhealthy Nodes: 0</span><br><span class="line">Rebooted Nodes : 0</span><br></pre></td></tr></table></figure><ul><li>source code<ul><li><a href="https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/MetricsOverviewTable.java#L68-L78" target="_blank" rel="noopener">MetricsOverviewTable</a></li><li><a href="https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/ClusterMetricsInfo.java" target="_blank" rel="noopener">ClusterMetricsInfo</a></li></ul></li><li>Apps Submited:<ul><li>Total apps <a href="https://github.com/apache/hadoop/blob/84e22a6af46db2859d7d2caf192861cae9b6a1a8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueMetrics.java#L60" target="_blank" rel="noopener">submitted</a>. When an application is submitted, it increases. This count never decreases.</li></ul></li><li>Apps pending<ul><li>Total apps in the pending status. When an application attempt is submitted, this count inreases. When an application attempt becomes running/finished/… from the pending status, this count decreases.</li><li>An application(RMApp) can have multiple app attempts based on <a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn.resourcemanager.am.max-attempts</a>.</li></ul></li><li>Apps running<ul><li>Total apps in the running status. When an application attempt becomes running, this count increases. When an application attempt becomes finished/… from the running status, this count decreases.</li></ul></li><li>Apps Completed<ul><li>Total apps completed. Completed(successfully) + failed + killed.</li></ul></li><li>Containers Running<ul><li>Total running container number. When containers are allocated, this count increases, when containers are released, this count decreases.</li></ul></li><li>Memory Used<ul><li>Currently allocated memory. When containers are allocated and released, this count changes accordingly, basicly change value is container_size * container_number.</li></ul></li><li>Memory Total<ul><li>Total memory of the cluster. It is available memory + allocated memory.</li></ul></li><li>Memory reserved<ul><li>Currently reserved memory. This <a href="https://blog.cloudera.com/blog/2013/06/improvements-in-the-hadoop-yarn-fair-scheduler/" target="_blank" rel="noopener">article</a> explains the usage of reserved memory very well. It can be used to prevent an application from being starvation.</li></ul></li><li>Vcores Used/Total/Reserved<ul><li>This is similiar as memory.</li></ul></li><li>Active Nodes<ul><li>Number of nodes in the active status.</li></ul></li><li>Decommissioned Nodes<ul><li>Number of nodes in the decommissioning status. For more detailed info about decommission a node, refer <a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/bk_administration/content/ref-a179736c-eb7c-4dda-b3b4-6f3a778bd8c8.1.html" target="_blank" rel="noopener">this</a>.</li></ul></li><li>Lost nodes<ul><li>Number of nodes lost. Node has not sent a heartbeat for some configured time threshold are considered as lost.</li></ul></li><li>Unhealthy nodes<ul><li>Number of nodes that are unhealthy. Node manager sends heartbeat to the resource manager regularly and the heartbeat contains the healthy report of the node, then the resource manager can know which nodes are unhealthy.</li></ul></li><li>Rebooted nodes<ul><li>Number of nodes that are rebooted.</li></ul></li></ul></li><li><p>Scheduler Metrics</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Scheduler Type : Fair Scheduler</span><br><span class="line">Scheduling Resource Type : [MEMORY, CPU]</span><br><span class="line">Minimum Allocation : &lt;memory:256, vCores:1&gt;</span><br><span class="line">Maximum Allocation : &lt;memory:61440, vCores:32&gt;</span><br></pre></td></tr></table></figure><ul><li>Scheduler Type<ul><li>The scheduler used by the resource manager.</li></ul></li><li>Scheduling Resource Type<ul><li>Get all resource types information from known resource types.</li></ul></li><li>Minimum Allocation<ul><li>Get minimum allocatable resource.</li></ul></li><li>Maximum Allocation<ul><li>Get maximum allocatable resource at the cluster level.</li></ul></li></ul></li></ul><h2 id="About"><a href="#About" class="headerlink" title="About"></a>About</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Cluster ID:1549960821363</span><br><span class="line">ResourceManager state:STARTED</span><br><span class="line">ResourceManager HA state:active</span><br><span class="line">ResourceManager HA zookeeper connection state:ResourceManager HA is not enabled.</span><br><span class="line">ResourceManager RMStateStore:org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore</span><br><span class="line">ResourceManager started on:Tue Feb 12 08:40:21 +0000 2019</span><br><span class="line">ResourceManager version:2.7.3-...</span><br><span class="line">Hadoop version:2.7.3-...</span><br></pre></td></tr></table></figure><ul><li>Cluster ID<ul><li>This is the start timestamp of the resource manager.</li></ul></li><li>ResourceManager state<ul><li>The state of the resource manager, which might be one of NOTINITED, INITED, STARTED, STOPPED.</li></ul></li><li>ResourceManager HA state<ul><li>HA state of resource manager</li></ul></li><li>ResourceManager HA zookeeper connection state</li><li>ResourceManager RMStateStore<ul><li>RMStateStore is storage of ResourceManager state. Takes care of asynchronous notifications and interfacing with YARN objects. Real store implementations need to derive from it and implement blocking store and load methods to actually store and load the state.</li></ul></li><li>ResourceManager started on<ul><li>The start timestamp of resource manager</li></ul></li><li>ResourceManager version<ul><li>The version of resource manager</li></ul></li><li>Hadoop version<ul><li>The version of hadoop</li></ul></li></ul><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p>This shows a table of applications. One example row is as follows.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ID : application_1549960821363_10810</span><br><span class="line">User : 1</span><br><span class="line">Name : job1...</span><br><span class="line">Application Type: MAPREDUCE</span><br><span class="line">Queue : root.q1</span><br><span class="line">Starttime : Thu Feb 14 22:00:21 +0900 2019</span><br><span class="line">Finishtime : Thu Feb 14 22:00:34 +0900 2019</span><br><span class="line">State : FINISHED</span><br><span class="line">FinalStatus : SUCCEEDED</span><br><span class="line">Running Containers : N/A</span><br><span class="line">Tracking UI : History</span><br><span class="line">Blacklisted Nodes : N/A</span><br></pre></td></tr></table></figure></p><ul><li>source code<ul><li><a href="https://github.com/apache/hadoop/blob/88625f5cd90766136a9ebd76a8d84b45a37e6c99/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/webapp/AppsBlock.java#L150" target="_blank" rel="noopener">AppsBlock</a></li></ul></li><li>ID<ul><li>ApplicationId represents the globally unique identifier for an application. The globally unique nature of the identifier is achieved by using the cluster timestamp i.e. start-time of the ResourceManager along with a monotonically increasing counter<br>for the application.</li></ul></li><li>User<ul><li>The user who submitted the application.</li></ul></li><li>Name<ul><li>Get the user-defined name of the application.</li></ul></li><li>State<ul><li>Enumeration of various <a href="https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/YarnApplicationState.java" target="_blank" rel="noopener">states of an ApplicationMaster</a>.</li></ul></li><li>Running Containers<ul><li>The number of containers.</li></ul></li><li>Tracking UI<ul><li>The tracking url for the application.</li></ul></li><li>Blacklisted Nodes<ul><li>A blacklisted node to indicate that a node is unhealthy and hadoop should not assign any tasks to it anymore.</li></ul></li></ul><h2 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h2><p>This is a graph showing the current usage status of queues. </p><p>Take fair scheduler as a example. This graph tells us what are the queues, how much resource can a queue use up on fair, how much resource of a queue has been used up, is a queue used more than fair/less than fair, which queue is busy/idle, and so on.</p><h1 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h1><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>This is actually redirects to http://{hadoop-master-ip}:8088/conf</p><p>This shows all configurations from the source of hdfs-default.xml, yarn-default.xml, mapred-default.xml, core-default.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml, core-site.xml,</p><h2 id="Local-logs"><a href="#Local-logs" class="headerlink" title="Local logs"></a>Local logs</h2><p>yarn logs</p><h2 id="Server-stacks"><a href="#Server-stacks" class="headerlink" title="Server stacks"></a>Server stacks</h2><p>Stack trace of the resource manager.</p><h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>JMX metrics of the resource manager.</p><h1 id="Check-job-detailed-information"><a href="#Check-job-detailed-information" class="headerlink" title="Check job detailed information"></a>Check job detailed information</h1><p>This is a complex part, it is not as straightforward as other parts because there are many logs and UIs involved.<br>Yarn log aggregation needs to be enabled for checking the logs. (<a href="https://tiana528.github.io/2018/11/19/hive-configuration-best-practise/">Enable yarn log aggregation</a>)</p><h2 id="Check-application-detailed-info"><a href="#Check-application-detailed-info" class="headerlink" title="Check application detailed info"></a>Check application detailed info</h2><p>Click application id such as <code>application_1542608710651_1268349</code>, then the application UI will open. URL is : {master_ip:8088}/cluster/app/application_1549418419376_658006</p><p>The bottom part shows the application attempts if there are any. If an application attemp fails, another application attempt will be triggered until the attempt number meets the configured maximum value. We can know how many times of the application attempts have been tried/trying from this UI.</p><p>It is able to check the logs of the corresponding application master by clicking the <code>logs</code> in each row.</p><h2 id="Check-application-attempt-detailed-info"><a href="#Check-application-attempt-detailed-info" class="headerlink" title="Check application attempt detailed info"></a>Check application attempt detailed info</h2><p>Click <code>Tracking URL: ApplicationMaster</code></p><ul><li>If the application master hasn’t been started, the UI will be redirected to the application detailed info UI.</li><li><p>If the application master is running, the URL is like : {master_ip}:8088/proxy/application_1549418419376_657056/mapreduce/job/job_1549418419376_657056</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Task Type :  Map, Reduce</span><br><span class="line">Progress: ...</span><br><span class="line">Total  : 2434, 999</span><br><span class="line">Pending : 0, 871</span><br><span class="line">Running : 0, 63</span><br><span class="line">Complete : 2434, 65</span><br><span class="line"></span><br><span class="line">Attempt Type : Maps, Reduces</span><br><span class="line">New : 0, 871</span><br><span class="line">Running : 0, 63</span><br><span class="line">Failed : 0, 0</span><br><span class="line">Killed : 0, 0</span><br><span class="line">Successful : 2434, 65</span><br></pre></td></tr></table></figure><ul><li>The above part(Task type) shows the total number of map/reduce tasks and how many are running/pending/completed.</li><li>The blow part(Attempt type) shows the info of attempts. How many attemps are new/running/failed/killed/successful. </li></ul></li><li><p>If the application master has finished, the UI will be redirected to jobhistory. The URL is like : {job_history_server_ip}:19888/jobhistory/job/job_1549418419376_658293. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Task Type :  Map, Reduce</span><br><span class="line">Total  : 12328, 0</span><br><span class="line">Complete : 0, 0</span><br><span class="line"></span><br><span class="line">Attempt Type : Maps, Reduces</span><br><span class="line">Failed : 597, 0</span><br><span class="line">Killed : 209, 0</span><br><span class="line">Successful : 0, 0</span><br></pre></td></tr></table></figure><ul><li>The above part(Task type) shows the total number of map/reduce tasks and how many have been finished.</li><li>The blow part(Attempt type) shows the info of attempts. How many attemps are failed/killed/successful. </li></ul></li></ul><h2 id="Check-task-attempt-detailed-information"><a href="#Check-task-attempt-detailed-information" class="headerlink" title="Check task attempt detailed information"></a>Check task attempt detailed information</h2><p>Click the links in the jobs UI, then it will redirect to a list of tasks, and click task_id(e.g. task_1549418419376_657056_r_000732), the detailed info UI of a task attempt will open.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Attempt : attempt_1549418419376_657056_r_000732_0</span><br><span class="line">Progress : 76.90</span><br><span class="line">State: Running</span><br><span class="line">Status : reduce &gt; reduce</span><br><span class="line">Node : 172.1.1.2:8042</span><br><span class="line">Logs : logs</span><br><span class="line">Started : Sat Feb 16 11:57:37 +0900 2019</span><br><span class="line">Finished : N/A</span><br><span class="line">Elapsed : 42sec</span><br><span class="line">Note : </span><br><span class="line">Actions : kill</span><br></pre></td></tr></table></figure></p><ul><li>Attempt : attempt id</li><li>Progress : the progress of the current task attempt</li><li>State : the state of the current task attempt</li><li>Status : what is the status of the current task attempt<ul><li>If it is a map task attempt, it will show <code>SCHEDULED</code>, <code>...&gt; map</code>, <code>... &gt; sort</code>. They are different phrases of the map process.</li><li>If it is a reducer task attempt, it will show <code>SCHEDULED</code>, <code>35 / 35 copied.</code>, <code>reduce &gt; sort</code>, <code>reduce &gt; reduce</code>. They are different phrases of the reduce process.</li></ul></li><li>Logs : <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stderr : Total file length is 243 bytes.</span><br><span class="line">stdout : Total file length is 0 bytes.</span><br><span class="line">syslog : Total file length is 79129 bytes.</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HDFS UI detailed description</title>
      <link href="/2019/02/09/HDFS_UI_detailed_description/"/>
      <url>/2019/02/09/HDFS_UI_detailed_description/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS-UI"><a href="#HDFS-UI" class="headerlink" title="HDFS UI"></a>HDFS UI</h1><p>Open HDFS UI by browser, the URL is {hadoop_master_ip:50070}.</p><p>This article aims at explaining everything in the HDFS Overview UI in detail, other tabs content is self-described.</p><p>This article is based on hadoop of version 2.7.3.</p><p>(A lot of information can be get from FSNameSystem.)</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Started:Sun Feb 10 09:33:34 UTC 2019</span><br><span class="line">Version:2.7.3, ...</span><br><span class="line">Compiled:2018-11-16T09:23Z by root from ...</span><br><span class="line">Cluster ID:CID-c489b321-4d13-423b-a7e9-e8f66355e17a</span><br><span class="line">Block Pool ID:BP-5769292-172.18.204.140-1549791212026</span><br></pre></td></tr></table></figure><ul><li>Source code<ul><li><a href="https://github.com/apache/hadoop/blob/release-2.7.3-RC2/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/dfshealth.html" target="_blank" rel="noopener">dfshealth.html</a></li></ul></li><li>Explanation<ul><li>Started<ul><li>The starttime of the namenode.</li></ul></li><li>Version<ul><li>Hadoop version.</li></ul></li><li>Compiled<ul><li>Haodop compiled information.</li></ul></li><li>Cluster ID<ul><li>As explained in <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">hdfs federation</a>, a ClusterID identifier is used to identify all the nodes in the cluster. When a Namenode is formatted, this identifier is either provided or auto generated.</li><li>When formatting the namenode of an running hadoop cluster, a new ClusterID will be generated, it is also needed to change the <code>usr/local/hadoop/dfs/datanode/current/VERSION</code> file to update the value of ClusterID on all slaves. </li></ul></li><li>Block Pool ID:<ul><li>As explained in <a href="https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html" target="_blank" rel="noopener">hdfs federation</a>, a Block Pool is a set of blocks that belong to a single namespace.</li></ul></li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Security is off.</span><br><span class="line"></span><br><span class="line">Safemode is off.</span><br><span class="line"></span><br><span class="line">181 files and directories, 60 blocks = 241 total filesystem object(s).</span><br><span class="line"></span><br><span class="line">Heap Memory used 254 MB of 401.13 MB Heap Memory. Max Heap Memory is 9.94 GB.</span><br><span class="line"></span><br><span class="line">Non Heap Memory used 59.1 MB of 60.06 MB Commited Non Heap Memory. Max Non Heap Memory is -1 B.</span><br><span class="line"></span><br><span class="line">Configured Capacity:899.56 GB</span><br><span class="line">DFS Used:2.02 GB (0.23%)</span><br><span class="line">Non DFS Used:3.97 GB</span><br><span class="line">DFS Remaining:893.56 GB (99.33%)</span><br><span class="line">Block Pool Used:2.02 GB (0.23%)</span><br><span class="line">DataNodes usages% (Min/Median/Max/stdDev):0.06% / 0.31% / 0.31% / 0.12%</span><br><span class="line">Live Nodes3 (Decommissioned: 0)</span><br><span class="line">Dead Nodes0 (Decommissioned: 0)</span><br><span class="line">Decommissioning Nodes0</span><br><span class="line">Total Datanode Volume Failures0 (0 B)</span><br><span class="line">Number of Under-Replicated Blocks0</span><br><span class="line">Number of Blocks Pending Deletion2</span><br><span class="line">Block Deletion Start Time2/10/2019, 6:33:34 PM</span><br></pre></td></tr></table></figure><ul><li>Security<ul><li>Whether hadoop <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html" target="_blank" rel="noopener">security mode</a> is on or off. When Hadoop is configured to run in secure mode, each Hadoop service and each user must be authenticated by Kerberos.</li></ul></li><li>Safemode<ul><li>Whether <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Safemode" target="_blank" rel="noopener">safemode</a> is on or off. Safemode for the NameNode is essentially a read-only mode for the HDFS cluste</li><li></li></ul></li><li>181 files<ul><li>The current size of inodes in HDFS. <a href="https://github.com/apache/hadoop/blob/45caeee6cfcf1ae3355cd880402159cf31e94a8a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#L4844" target="_blank" rel="noopener">Link</a></li></ul></li><li>60 blocks<ul><li>The current number of blocks. <a href="https://github.com/apache/hadoop/blob/45caeee6cfcf1ae3355cd880402159cf31e94a8a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java#L6017" target="_blank" rel="noopener">Link</a></li></ul></li><li>Memory related<ul><li>This part describes namenode memory information in the following format.<ul><li>used {used} MB of {committed} MB. Max Memory is {max}</li></ul></li><li>Refer to <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/management/MemoryUsage.html" target="_blank" rel="noopener">MemoryUsage</a> for the definition of used, committed, and max.</li><li>init is near xms, max is near xmx, used is actual being used, committed is garranteed by os. committed &gt;= used. There can be OutOfMemoryError if max &gt; used + allocating_memory &gt; committed, when os virtual memory is insufficient.</li></ul></li><li>Configured Capacity<ul><li>Total raw capacity of data nodes in bytes. <a href="https://github.com/apache/hadoop/blob/a55d6bba71c81c1c4e9d8cd11f55c78f10a548b0/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeStats.java#L36" target="_blank" rel="noopener">Link</a></li></ul></li><li>DFS Used<ul><li>The percentage of the used capacity over the total capacity.</li></ul></li><li>Non DFS Used<ul><li>The total used space by data nodes for non-DFS purposes such as storing temporary files on the local file system.</li></ul></li><li>DFS Remaining<ul><li>The remaining capacity.</li></ul></li><li>Block Pool Used<ul><li>The used space by the block pool on all data nodes.</li></ul></li><li>DataNodes usages<ul><li>Min/Median/Max/stdDev refers to the minimum/median/maximum/standard_deviation of the dfs used space percentage by all datanodes.</li></ul></li><li>Live Nodes<ul><li>Number of datanodes which are currently live.</li></ul></li><li>Dead Nodes<ul><li>Number of datanodes which are currently dead.</li></ul></li><li>Decommissioning Nodes<ul><li>Number of datanodes where decommissioning is in progress.</li></ul></li><li>Total Datanode Volume Failures<ul><li>Total number of volume failures across all Datanodes.</li></ul></li><li>Number of Under-Replicated Blocks<ul><li>Get aggregated count of all blocks with low redundancy.</li></ul></li><li>Number of Blocks Pending Deletion<ul><li>The total number of blocks to be invalidated.</li></ul></li><li>Block Deletion Start Time<ul><li>The timestamp of bock deletion start.</li></ul></li></ul><h2 id="NameNode-Journal-Status"><a href="#NameNode-Journal-Status" class="headerlink" title="NameNode Journal Status"></a>NameNode Journal Status</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Current transaction ID: 278965</span><br><span class="line"></span><br><span class="line">Journal Manager: FileJournalManager(root=/.../hdfs/name)</span><br><span class="line">State : EditLogFileOutputStream(/.../hdfs/name/current/edits_inprogress_0000000000000000001)</span><br></pre></td></tr></table></figure><ul><li>Current transaction ID<ul><li>The last transaction ID that was either loaded from an image or loaded by loading edits files. Note that this is not precise value and can only be used by metrics.</li></ul></li><li>Journal Manager<ul><li>The class of Jourmnal Manager</li><li>A <a href="https://github.com/apache/hadoop/blob/fac9f91b2944cee641049fffcafa6b65e0cf68f2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalManager.java" target="_blank" rel="noopener">JournalManager</a> is responsible for managing a single place of storing edit logs. It may correspond to multiple files, a backup node, etc. Even when the actual underlying storage is rolled, or failed and restored, each conceptual place of storage corresponds to exactly one instance of this class, which is created when the EditLog is first opened.</li></ul></li><li>State:<ul><li>A short text snippet suitable for describing the current status of the EditLogOutputStream(EditLogOutputStream is for supporting journaling of edits logs into a persistent storage).</li></ul></li></ul><h2 id="NameNode-Storage"><a href="#NameNode-Storage" class="headerlink" title="NameNode Storage"></a>NameNode Storage</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NameNode Storage</span><br><span class="line">Storage Directory : /.../hdfs/name</span><br><span class="line">Type : IMAGE_AND_EDITS</span><br><span class="line">State : Active</span><br></pre></td></tr></table></figure><ul><li>Storage Directory<ul><li>The storage directory for namenode</li></ul></li><li>Type<ul><li>StorageDirType specific to namenode storage. A Storage directory could be of type IMAGE which stores only fsimage, or of type EDITS which stores edits or of type IMAGE_AND_EDITS which stores both fsimage and edits.</li></ul></li><li>State<ul><li>Status of namenode.</li></ul></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Basic understanding of debian packaging</title>
      <link href="/2019/02/07/Basic_understanding_of_debian_packaging/"/>
      <url>/2019/02/07/Basic_understanding_of_debian_packaging/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>It never becomes easy to setup/install/upgrade environments. This article aims at describing how to use/create debian packages to help setup the environments.</p><h2 id="What-is-a-debian-package"><a href="#What-is-a-debian-package" class="headerlink" title="What is a debian package"></a>What is a debian package</h2><ul><li>Linux<ul><li><a href="https://en.wikipedia.org/wiki/Linux" target="_blank" rel="noopener">Linux</a> is a family of free and open-source software operating systems based on the Linux kernel.</li></ul></li><li>Linux kernel<ul><li><a href="https://en.wikipedia.org/wiki/Linux_kernel" target="_blank" rel="noopener">Linux kernel</a> is a free and open-source, monolithic, Unix-like operating system kernel.</li></ul></li><li>Linux distribution<ul><li>roughly : Linux distribution = Linux kernel + software</li><li>A <a href="https://en.wikipedia.org/wiki/Linux_distribution" target="_blank" rel="noopener">Linux distribution</a> is an operating system made from a software collection, which is based upon the Linux kernel and, often, a package management system. There are more than 600 kinds of linux distributions.</li></ul></li><li>Debian<ul><li>Debian is one of the Linux distributions, also called Debian GNU/Linux.</li><li><a href="https://en.wikipedia.org/wiki/Debian" target="_blank" rel="noopener">Debian</a> is one of the earliest operating systems based on the Linux kernel, and containing more than 51000 <a href="https://www.debian.org/index.en.html" target="_blank" rel="noopener">debian packges</a></li></ul></li><li>Debian package<ul><li>A <a href="https://wiki.debian.org/Packaging" target="_blank" rel="noopener">Debian package</a> is a collection of files that allow for applications or libraries to be distributed via the Debian package management system. </li></ul></li><li>Relationship between debian packages and linux distributions<ul><li><a href="https://en.wikipedia.org/wiki/List_of_Linux_distributions" target="_blank" rel="noopener">Linux distributions</a> can be divided into several categories according to how packages are managed. PRM-based(.rpm file) distributions includes Red Hat Linux, CentOS, Fedora and so on. Debian-based(.deb file) distributions includes Ubuntu, GNU/Linux and so on.</li></ul></li></ul><h1 id="Create-debian-packages"><a href="#Create-debian-packages" class="headerlink" title="Create debian packages"></a>Create debian packages</h1><p>It is possible to use the official tool(dpkg-deb) to create debian packages. But here we use <a href="https://github.com/jordansissel/fpm" target="_blank" rel="noopener">fpm</a> to create debian packages. It is very simple to use.</p><p>Refer <a href="https://fpm.readthedocs.io/en/latest/installing.html" target="_blank" rel="noopener">this</a> to install fpm.</p><h3 id="Example1"><a href="#Example1" class="headerlink" title="Example1"></a>Example1</h3><p>Create a debian package whose name is “hello_world”, and what it do is installing a file “hello world” in the path of /tmp/hi.txt</p><p>It is very easy, just execute 3 commands.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p working_folder/tmp</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world"</span> &gt; working_folder/tmp/hi.txt</span><br><span class="line">fpm \</span><br><span class="line">  --architecture=amd64 \</span><br><span class="line">  --<span class="built_in">chdir</span>=working_folder \</span><br><span class="line">  --input-type=dir \</span><br><span class="line">  --output-type=deb \</span><br><span class="line">  --name=hello_world \</span><br><span class="line">  --version=<span class="string">"0.0.1"</span> \</span><br><span class="line">  --iteration=<span class="string">"001"</span> \</span><br><span class="line">  --description=<span class="string">"This is an example"</span> \</span><br><span class="line">  --deb-user=<span class="string">"root"</span> \</span><br><span class="line">  --deb-group=<span class="string">"root"</span> \</span><br><span class="line">  --deb-priority=<span class="string">"extra"</span> \</span><br><span class="line">  <span class="string">"."</span></span><br></pre></td></tr></table></figure><p>Then a file of hello-world_0.0.1-001_amd64.deb will be created in the current folder.<br>Use <code>sudo dpkg -i hello-world_0.0.1-001_amd64.deb</code> to install the deb file, and then you can see a file(/tmp/hi.txt) containing “hello world” is created.</p><h2 id="Example2"><a href="#Example2" class="headerlink" title="Example2"></a>Example2</h2><p>This is an example to create a symlink /etc/link which links to /tmp/hi.txt. This example shows the usage of <code>--after-install</code> to do something at the end of package installation.</p><p>Create a file hook-AfterInstall.sh.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env bash</span><br><span class="line">ln -s /tmp/hi.txt /etc/link</span><br></pre></td></tr></table></figure></p><p>Then run 3 commands.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p working_folder/tmp</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world"</span> &gt; working_folder/tmp/hi.txt</span><br><span class="line">fpm \</span><br><span class="line">  --architecture=amd64 \</span><br><span class="line">  --<span class="built_in">chdir</span>=working_folder \</span><br><span class="line">  --input-type=dir \</span><br><span class="line">  --output-type=deb \</span><br><span class="line">  --name=hello_world \</span><br><span class="line">  --version=<span class="string">"0.0.1"</span> \</span><br><span class="line">  --iteration=<span class="string">"002"</span> \</span><br><span class="line">  --description=<span class="string">"This is an example"</span> \</span><br><span class="line">  --deb-user=<span class="string">"root"</span> \</span><br><span class="line">  --deb-group=<span class="string">"root"</span> \</span><br><span class="line">  --deb-priority=<span class="string">"extra"</span> \</span><br><span class="line">  --after-install=<span class="string">"hook-AfterInstall.sh"</span> \</span><br><span class="line">  <span class="string">"."</span></span><br></pre></td></tr></table></figure></p><p>Then hello-world_0.0.1-002_amd64.deb is created in the current folder. Installing it will create a symlink.</p><h1 id="Debian-package-management-tools"><a href="#Debian-package-management-tools" class="headerlink" title="Debian package management tools"></a>Debian package management tools</h1><h2 id="dpkg"><a href="#dpkg" class="headerlink" title="dpkg"></a>dpkg</h2><ul><li>See help<ul><li>dpkg –help</li><li>dpkg –force-help</li></ul></li><li>Print out the control file and other information for a specified package<ul><li>dpkg –info foo.deb</li></ul></li><li>Install a package onto the file system of the hard disk<ul><li>dpkg –install foo_VVV-RRR.deb</li></ul></li><li>List the files in a package deb file<ul><li>dpkg -c foo.deb</li></ul></li><li>Remove a package (but not its configuration files)<ul><li>dpkg –remove foo</li></ul></li><li>Remove a package (including its configuration files)<ul><li>dpkg –purge foo</li></ul></li><li>Remove a package no matter that it is depended by other existing packages<ul><li>dpkg –purge –force-depends foo</li></ul></li><li>List all files ‘owned’ by a package<ul><li>dpkg –listfiles foo</li></ul></li><li>Find packages by file name.<ul><li>dpkg –search foo*</li></ul></li><li>Find packages by package name.<ul><li>dpkg –list foo*</li></ul></li></ul><h2 id="aptitude"><a href="#aptitude" class="headerlink" title="aptitude"></a>aptitude</h2><p>aptitude provides the functionality of apt-get</p><ul><li>Install a new package<ul><li>apt-get install foo</li></ul></li><li>Install a new package by force<ul><li>apt-get -f install</li></ul></li><li>Synchronize the package index files from their sources<ul><li>apt-get update</li></ul></li><li>Remove packages without configurations<ul><li>apt-get remove foo</li></ul></li><li>Remove packages with configurations<ul><li>apt-get purge foo</li></ul></li><li>Search packages<ul><li>apt-cache search foo<ul><li>Lists packages whose name or description contains foo</li><li>This queries and displays available information about installed and installable packages.</li></ul></li></ul></li></ul><h2 id="dpkg-deb"><a href="#dpkg-deb" class="headerlink" title="dpkg-deb"></a>dpkg-deb</h2><ul><li>Determine what files are contained in a Debian archive file<ul><li>dpkg-deb –contents foo.deb</li></ul></li></ul><h1 id="Common-questions"><a href="#Common-questions" class="headerlink" title="Common questions"></a>Common questions</h1><ul><li>What are <a href="https://www.debian.org/doc/manuals/maint-guide/dother.en.html#conffiles" target="_blank" rel="noopener">conffiles</a>?<ul><li>conffiles are just configuration files of a package. When you upgrade a package, you’ll be asked whether you want to keep your old configuration files or not.</li><li>Files under the /etc directory are marked as conffiles automatically.</li><li>If your program uses configuration files but also rewrites them on its own, it’s best not to make them conffiles because dpkg will then prompt users to verify the changes all the time.</li><li>If the program you’re packaging requires every user to modify the configuration files in the /etc directory, there are two popular ways to arrange for them to not be conffiles, keeping dpkg quiet:<ul><li>Create a symlink under the /etc directory pointing to a file under the /var directory generated by the maintainer scripts.</li><li>Create a file generated by the maintainer scripts under the /etc directory.</li></ul></li></ul></li><li>What packages are already installed?<ul><li>dpkg –list</li><li>dpkg –list ‘foo*’</li></ul></li><li>How to check detail of installed packages<ul><li>dpkg –status foo</li></ul></li><li>How to display the files on an installed package?<ul><li>dpkg –listfiles foo</li></ul></li><li>How can I find out what package produced a particular file?<ul><li>dpkg –search foo<ul><li>This will search all of the files having the file extension of .list in the directory /var/lib/dpkg/info/.</li></ul></li></ul></li><li>Can multiple versions of the same package exist on the same system?<ul><li>No.</li></ul></li><li>What happens when upgrade a binary debian package?<ul><li>It will first delete all files created by the old version except conffiles, then install the new version. So there might be a small period that there will be no available/executable package.</li></ul></li><li>What are conffiles of an installed packages?<ul><li>ls /var/lib/dpkg/info/. | grep conffiles</li></ul></li><li>How to install packages in place without breaking existing running service?<ul><li>As we discribed before, updating a package will first delete all files owned by the current installed version, then install the new version. There is a small time window during the package versionup that there is no available/executable package. This should be considered when performing environment update. One solution is rolling update, never let a service in the middle of update provides actual service. The other is always installing new packages and do not update existing packages.</li></ul></li><li>How to downgrade the version of a package?<ul><li>apt get install foo=1.2 –allow-downgrades</li></ul></li><li>How to dealwith conffiles during package version up?<ul><li><a href="https://raphaelhertzog.com/2010/09/21/debian-conffile-configuration-file-managed-by-dpkg/" target="_blank" rel="noopener">Everything you need to know about conffiles: configuration files managed by dpkg</a></li></ul></li></ul><h1 id="Scenarios"><a href="#Scenarios" class="headerlink" title="Scenarios"></a>Scenarios</h1><p>Here we discuss some scenarios.</p><ul><li><p>version up packages and revert the version up</p><ul><li>Use <code>--allow-downgrades</code> option to downgrade a version(revert a release) without prompting.<ul><li>e.g. current version of package p1 is 1.2, then run <code>sudo apt-get install p1=1.1 --allow-downgrades</code></li></ul></li></ul></li><li><p>check broken installed packages</p><ul><li>Run <code>sudo apt-get update</code></li></ul></li><li><p>override a file installed by another debian packages</p><ul><li>Never do this. It is not maintainable. However, sometimes, we want to move some files(responsibilities) from one package to another package, and in such case, we need to release two packages together in the specific order.<ul><li>e.g. Package p1 install file f1, we want to let package p2 install the file, then implement it and release new version of p1 and p2. During deployment, we first install new version of p1, which will delete the file f1 installed by the previous version, then install new version of p2 to avoid confliction.</li></ul></li></ul></li><li><p>installation of some packages fail in the middle</p><ul><li>This leaves a <code>held package</code>, detect a held package by <code>sudo dpkg --get-selections | grep held</code>.</li><li>If the broken package is caused by ramdomly issue, a retry might help fix the issue, run <code>sudo dpkg --configure -a</code>.</li><li>If the broken package is caused by package script issue, and needs to be fixed by installing a new version, then perform cleanup and install new version. <code>sudo apt-get remove p1</code> and <code>sudo apt-get install p1=...</code></li></ul></li><li><p>list up local installed debian packages</p><ul><li>sudo dpkg -l</li></ul></li><li><p>search remote installable debian packages</p><ul><li>sudo apt-cache search foo</li></ul></li></ul><ul><li>install cron job<ul><li>It is very common to create a crob job in a debian package, just create a file under /etc/cron.d instead of using the crontab command. Note that it is not good to use crontab command because if multiple packages use the crontab command to install cron jobs, they will all edit the same file and is hard to maintenance. Instead, copying a file under /etc/cron.d enables each package manages its cron jobs independently and isolately.</li></ul></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://www.debian.org/doc/manuals/debian-faq/ch-pkgtools.en.html" target="_blank" rel="noopener">FAQ</a> of dpkg tools</li><li>How to use <a href="https://help.ubuntu.com/community/AptGet/Howto" target="_blank" rel="noopener">apt-get</a>?</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> debian package </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop cluster incidents</title>
      <link href="/2019/01/25/hadoop-cluster-incidents/"/>
      <url>/2019/01/25/hadoop-cluster-incidents/</url>
      
        <content type="html"><![CDATA[<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><p>The operation of Hadoop cluster is not easy. There are many parameters and there is no perfect configuration that fit all situations. Monitoring and learn from each incidents is very important. This article aims at summarizing some incidents and they can be seen as valued knowledge to help investigate future incidents and discuss approaches to prevent. </p><h1 id="datanode-Xmx-configured-does-not-fit-the-instance"><a href="#datanode-Xmx-configured-does-not-fit-the-instance" class="headerlink" title="datanode Xmx configured does not fit the instance"></a>datanode Xmx configured does not fit the instance</h1><ul><li>situation<ul><li>The whole cluster performance degrades.</li><li>One node in the cluster has extremely higher CPU utilization than other nodes.</li><li>The datanode process on that node takes up more than 300% CPU by checking top command.</li><li>The accumulate GC time of the datanode processs on that node increases rapidly(know it through datadog jvm.gc.parnew.time metrics).</li></ul></li><li>analysis<ul><li>The cluster performance degrades because one datanode process behaves bad.</li><li>GC time increases rapidly indicates the memory is insufficient.</li></ul></li><li>root cause<ul><li>The Xmx parameter of the datanode process is configured too large. The slave instance type is aws c4.2xlarge which is 15GB, 8vCPU. Each core(map,reduce,application master) Xmx is configured as 2GB, node manager Xmx is also 2GB, but datanode JMX is configured as 10GB. Meaning that the datanode Xmx configured is much higher than the free memory on the node thus causing frequent memory swap and performance degradation, as well as longer GC time. This articles explains a little more about <a href="http://www.javaperformancetuning.com/news/qotm045.shtml" target="_blank" rel="noopener">the situation when a large heap(Xmx) is configured</a>.</li></ul></li><li>learn<ul><li>Monitoring GC time increasing speed helps find the situations that the JVM paramers are mis-configured. Free space on the node should be considered when deciding the Xmx value.</li></ul></li></ul><h1 id="to-be-continued"><a href="#to-be-continued" class="headerlink" title="to be continued"></a>to be continued</h1>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Very basic understanding of CAP theorem</title>
      <link href="/2019/01/19/basic-understanding-of-cap-theorem/"/>
      <url>/2019/01/19/basic-understanding-of-cap-theorem/</url>
      
        <content type="html"><![CDATA[<h1 id="Wiki-description"><a href="#Wiki-description" class="headerlink" title="Wiki description"></a>Wiki description</h1><p>According to the <a href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener">wiki</a>: CAP theorem explains that it is impossible for a <em>distributed data store</em> to simultaneously provide more than two out of the following three guarantees:</p><ul><li>Consistency: Every read receives the most recent write or an error.</li><li>Availability: Every request receives a (non-error) response – without the guarantee that it contains the most recent write.</li><li>Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.</li></ul><h1 id="Simple-understanding"><a href="#Simple-understanding" class="headerlink" title="Simple understanding"></a>Simple understanding</h1><p>Here explain the theorem using a simple example. Considering there are two nodes, and data is replicated in two nodes.</p><ul><li>If AP, then not C. When network partition happens(P), a write request arrives at one node and success(A), then the data on the two nodes must be inconsistent, meaning C is not satisfied.</li><li>If CP, then not A. When network partition happens(P), the read on either node always gets the latest written value. Then write operation must be forbidden on both side of the parititons, meaning A is not satisfied.</li><li>IF AC, then not P. If write always successes(A), read always gets the recent written value(C), then network partition must not happen, meaning P is not satisfied.</li></ul><h1 id="Does-it-explain-well"><a href="#Does-it-explain-well" class="headerlink" title="Does it explain well"></a>Does it explain well</h1><p>The above understanding of CAP is not sufficient. CAP is always criticized for being too simplistic and often misleading. Actually, many distributed data systems are not even CP, AP or AC, considering the strict definition. It is difficult for people to understand what is CAP by the above simple description which is explained in a lot of places on the Internet. The following of this article aims at <strong>adding a little more about the missing part of the above CAP theorem description and make it understandable</strong>.</p><h1 id="Definition-of-terms"><a href="#Definition-of-terms" class="headerlink" title="Definition of terms"></a>Definition of terms</h1><p>Before going any further, we need to make precise definition of terms. There are many articles discussing CAP, without clear definition and the readers may not have enough knowledge to understand them.</p><h2 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h2><ul><li>serialization is a transaction concept. Multiple transactions are processed in parallel, and each transaction contains one or more instructions. The database needs to <a href="https://en.wikipedia.org/wiki/Schedule_(computer_science)" title="[external] [title]" target="_blank" rel="noopener">schedule</a> how the instructions are executed. Note that different execution orders may result in different result.<ul><li>For example, start with the status a = 1, b = 3<ul><li>t1<ul><li>i1 : t1 = a</li><li>i2 : b = t1 + 1</li></ul></li><li>t2<ul><li>i3 : t2 = b</li><li>i4 : a = t2 + 1</li></ul></li><li>Execution order of i1, i3, i2, i4 results in a = 4, b = 2. Execution order of i1, i2, i3, i4 results in a = 3, b = 2.</li></ul></li></ul></li><li>A transaction schedule is <em>serializable</em> if the outcome is as if transactions are executed atomically and in some sequential order.</li><li>Strict serializability : Serializability + transactions are processed in the “real-time” order.</li></ul><h2 id="How-to-garrantee-serialization"><a href="#How-to-garrantee-serialization" class="headerlink" title="How to garrantee serialization"></a>How to garrantee serialization</h2><ul><li>Using concurrency control protocol  such as <a href="https://en.wikipedia.org/wiki/Two-phase_locking" target="_blank" rel="noopener">2PL</a><ul><li>2PL : 2 phrase locking<ul><li>acquire locks in first phrase, release locks in second phrase</li></ul></li><li>C2PL : Conservative two-phase locking<ul><li>acquire all locks at the beginning of the transaction, release locks in the second phrase.</li></ul></li><li>S2PL : Strict two-phase locking<ul><li>acquire locks in first phrase, release write locks at the end of the  transaction.</li></ul></li><li>SS2PL : Strong strict two-phase locking<ul><li>acquire locks in first phrase, release both read and write locks at the end of the transaction.</li><li>SS2PL has been the concurrency control protocol of choice for most database systems and utilized since their early days in the 1970s.</li></ul></li></ul></li><li>Using other approaches</li></ul><h2 id="Linearizability"><a href="#Linearizability" class="headerlink" title="Linearizability"></a>Linearizability</h2><ul><li><a href="https://en.wikipedia.org/wiki/Linearizability" target="_blank" rel="noopener">Linearizability</a> concept is from concurrent programming.  Linearizability is originally talking about single objects and not transactions. Nowadays, many papers talk about linearilzability transactions the same as strict serializability, and each transaction contains only one operation. </li><li>How Linearizability is related with CAP<ul><li>Linearizability is what the CAP Theorem calls Consistency.</li></ul></li></ul><h2 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h2><ul><li><a href="https://en.wikipedia.org/wiki/Isolation_(database_systems)" title="[external] [title]" target="_blank" rel="noopener">Phantom Read Phenomena</a><ul><li>Dirty reads<ul><li>A dirty read occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.</li></ul></li><li>Non-repeatable reads<ul><li>A non-repeatable read occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.</li></ul></li><li>Phantom reads<ul><li>A phantom read occurs when, in the course of a transaction, new rows are added or removed by another transaction to the records being read.</li></ul></li><li>Note that only these are insufficient and there are more situations.</li></ul></li><li>Levels(stronger to lower)<ul><li>Serializable<ul><li>This is the highest isolation level.</li><li>With a lock-based concurrency control DBMS implementation, serializability requires read and write locks (acquired on selected data) to be released at the end of the transaction. Also range-locks must be acquired when a SELECT query uses a ranged WHERE clause, especially to avoid the phantom reads phenomenon.</li></ul></li><li>Repeatable reads<ul><li>a lock-based concurrency control DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction. However, range-locks are not managed, so phantom reads can occur.</li></ul></li><li>Read committed<ul><li>a lock-based concurrency control DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the SELECT operation is performed (so the non-repeatable reads phenomenon can occur in this isolation level). As in the previous level, range-locks are not managed.</li></ul></li><li>Read uncommitted<ul><li>This is the lowest isolation level. In this level, dirty reads are allowed, so one transaction may see not-yet-committed changes made by other transactions.</li></ul></li><li>There are also incufficient and there are more that can be listed.</li></ul></li><li>How isolation is related in CAP<ul><li>Since the C(consistency) in CAP refers to linearizability which is very strong, and per transaction contains one operation, all actions happen instantaneously. The C(consistency) actually ensures the strongest isolation : serializable isolution level.</li></ul></li></ul><h2 id="What-isolation-level-are-commercial-databases-providing"><a href="#What-isolation-level-are-commercial-databases-providing" class="headerlink" title="What isolation level are commercial databases providing"></a>What isolation level are commercial databases providing</h2><ul><li>READ COMMITTED is defaulted isolation level on PostgreSQL, SQL Server, and Oracle.</li><li>REPEATABLE READ is defaulted isolation level on Mysql Innodb.</li></ul><p>We can see that serializable isolation level is not used at lease by default. It is too heavy and required more locks(If use lock as concurrency control), less concurrency, and less throughput. It also infers that in CAP, even if we can only choose 2 of them at the same time, C(strong consistency, linearizability, highest isolation level) is mostly not chosen. Most databases choose weaker consistency, which has better performance.</p><h1 id="Look-back-at-CAP-theorem"><a href="#Look-back-at-CAP-theorem" class="headerlink" title="Look back at CAP theorem"></a>Look back at CAP theorem</h1><ul><li>C<ul><li>definition<ul><li>A guarantee that every node in a distributed cluster returns the same, most recent, successful write. Consistency refers to every client having the same view of the data. There are various types of consistency models. Consistency in CAP (used to prove the theorem) refers to linearizability or sequential consistency, a very strong form of consistency.</li></ul></li><li>meaning<ul><li>Apparently, this is very strong consistency and there are no dirty reads, non-repeatable reads, or phantom reads. </li></ul></li></ul></li><li>A<ul><li>definition<ul><li>Every non-failing node returns a response for all read and write requests in a reasonable amount of time. The key word here is every. To be available, every node on (either side of a network partition) must be able to respond in a reasonable amount of time.</li></ul></li><li>meaning<ul><li>If some operation finishes exceeding an acceptable time(e.g. 30 seconds), it just means not available.</li><li>This definition of availability is also strong. Note that during network partition, even if you say the partition which contains the majority of nodes responses the client, it is not seen as achieve availability in CAP. It is called availability if the less node partition nodes also responses write/read to the client without error.</li></ul></li></ul></li><li>P<ul><li>definition<ul><li>The system continues to function and upholds its consistency guarantees in spite of network partitions. Network partitions are a fact of life. Distributed systems guaranteeing partition tolerance can gracefully recover from partitions once the partition heals.</li></ul></li><li>meaning<ul><li>This is unavoided in a distributed environment. Not only network parition happens, but also network connection timeout can be seen as parititon.</li></ul></li></ul></li></ul><h1 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h1><p>In a network partition, even if it is only posssible to make a choice between avaialbility or consistency. Many databases are not choosing any of them, since the consistency (linearizability) is so expensive. Actually, this leaves a space for the database designers/users to choose a level of balanced consistency and availability according to the use case. Whenever looking into a database, it will be a good point to check what level of A and C does it provides when P happens, even if it is not perfect A or C according to the CAP definition.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html" target="_blank" rel="noopener">please-stop-calling-databases-cp-or-ap</a></li><li><a href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="noopener">cap-twelve-years-later-how-the-rules-have-changed</a></li><li><a href="https://begriffs.com/posts/2017-08-01-practical-guide-sql-isolation.html" target="_blank" rel="noopener">practical-guide-sql-isolation</a></li><li><a href="https://irenezhang.net/blog/2015/02/01/consistency.html" target="_blank" rel="noopener">consistency</a></li><li><a href="https://project.inria.fr/epfl-Inria/files/2017/02/JadHamza-talk.pdf" target="_blank" rel="noopener">What is the strongest consistency that could be achieved<br>in a large scale distributed system?</a></li><li><a href="http://www.bailis.org/blog/linearizability-versus-serializability/" target="_blank" rel="noopener">linearizability-versus-serializability/</a></li><li><a href="https://dddpaul.github.io/blog/2016/03/17/linearizability-and-serializability/" target="_blank" rel="noopener">linearizability-and-serializability/</a></li><li><a href="https://cs.stackexchange.com/questions/41698/linearizability-and-serializability-in-context-of-software-transactional-memory" target="_blank" rel="noopener">linearizability-and-serializability-in-context-of-software-transactional-memory</a></li><li><a href="https://dzone.com/articles/understanding-the-cap-theorem" target="_blank" rel="noopener">understanding-the-cap-theorem</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive GenericUDF and DeferredJavaObject analysis</title>
      <link href="/2019/01/09/hive-DeferredJavaObject/"/>
      <url>/2019/01/09/hive-DeferredJavaObject/</url>
      
        <content type="html"><![CDATA[<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>This article aims at discussing how hive generic User-defined function(GenericUDF) works. In the java doc, it says GenericUDF can do short-circuit evaluations using DeferedObject. But what is short-circuit evaluation and how DeferedObject works?</p><h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>Basicly, use GenericUDF when the input or output is complex type, or input arguments have variable length.</p><p>GenericUDF use DeferedObject to pass arguments and achieve lazy-evaluation and short-circuiting.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">interface</span> <span class="title">DeferredObject</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">prepare</span><span class="params">(<span class="keyword">int</span> version)</span> <span class="keyword">throws</span> HiveException</span>;</span><br><span class="line">  <span class="function">Object <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> HiveException</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><h1 id="Source-code-analysis"><a href="#Source-code-analysis" class="headerlink" title="Source code analysis"></a>Source code analysis</h1><p>We run a sql <code>select abs(c1) from t1</code> and try to analysis how UDF is initialized and processed.</p><ul><li>Class structure and relationship<ul><li>SelectOperator<ul><li>ExprNodeEvaluator[] eval<ul><li>(actually it is ExprNodeGenericFuncEvaluator)</li></ul></li></ul></li><li>ExprNodeGenericFuncEvaluator<ul><li>ExprNodeEvaluator[] children; <ul><li>(actually it is [ExprNodeColumnEvaluator[Column[c1]]])</li></ul></li><li>GenericUDF.DeferredObject[] deferredChildren;<ul><li>(actually it is org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject and contains the ExprNodeEvaluator)</li></ul></li><li>GenericUDF.DeferredObject[] childrenNeedingPrepare;<ul><li>(actually it contains the same object as deferredChildren in this example)</li></ul></li><li>GenericUDF genericUDF;<ul><li>(actually it is GenericUDFAbs)</li></ul></li></ul></li></ul></li></ul><ul><li><p>During initialization</p><ul><li>Generate query plan tree<ul><li>SemanticAnalyzer.genPlan</li></ul></li><li><p>Create ExprNodeGenericFuncDesc based on genericUDFClass</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExprNodeGenericFuncDesc <span class="title">newInstance</span><span class="params">(GenericUDF genericUDF,</span></span></span><br><span class="line"><span class="function"><span class="params">      String funcText,</span></span></span><br><span class="line"><span class="function"><span class="params">      List&lt;ExprNodeDesc&gt; children)</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    ObjectInspector oi = genericUDF.initializeAndFoldConstants(childrenOIs);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Initialize GenericUDF</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.udf.generic.GenericUDF</span><br><span class="line"><span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initializeAndFoldConstants</span><span class="params">(ObjectInspector[] arguments)</span></span></span><br><span class="line"><span class="function">      <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line"></span><br><span class="line">    ObjectInspector oi = initialize(arguments);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span> <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Initial select operator</p><ul><li><p>Create ExprNodeGenericFuncEvaluator</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.SelectOperator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initializeOp</span><span class="params">(Configuration hconf)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">  ...</span><br><span class="line">  eval[i] = ExprNodeEvaluatorFactory.get(colList.get(i), hconf);</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ExprNodeGenericFuncEvaluator</span><span class="params">(ExprNodeGenericFuncDesc expr, Configuration conf)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    children = <span class="keyword">new</span> ExprNodeEvaluator[expr.getChildren().size()];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; children.length; i++) &#123;</span><br><span class="line">      ExprNodeDesc child = expr.getChildren().get(i);</span><br><span class="line">      ExprNodeEvaluator nodeEvaluator = ExprNodeEvaluatorFactory.get(child, conf);</span><br><span class="line">      children[i] = nodeEvaluator;</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Init ExprNodeGenericFuncEvaluator and create DeferredExprObject</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector rowInspector)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">     deferredChildren = <span class="keyword">new</span> GenericUDF.DeferredObject[children.length];</span><br><span class="line">     </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ExprNodeGenericFuncEvaluator.DeferredExprObject</span><br><span class="line"> DeferredExprObject(ExprNodeEvaluator eval, <span class="keyword">boolean</span> eager) &#123;</span><br><span class="line">      <span class="keyword">this</span>.eval = eval; <span class="comment">//(ExprNodeEvaluator[Column[c1]])</span></span><br><span class="line">      <span class="keyword">this</span>.eager = eager; (<span class="keyword">false</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>During processing data</p><ul><li><p>select operator process one row</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.SelectOperator</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object row, <span class="keyword">int</span> tag)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    output[i] = eval[i].evaluate(row); <span class="comment">//(eval[i] is actually ExprNodeGenericFuncEvaluator)</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Note that <code>row</code> is an LazyStruct object which holds data in binary format</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Object <span class="title">_evaluate</span><span class="params">(Object row, <span class="keyword">int</span> version)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> genericUDF.evaluate(deferredChildren);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    Object valObject = arguments[<span class="number">0</span>].get();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Note that arguments[0] is ExprNodeGenericFuncEvaluator&amp;DeferredExprObject</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator</span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">get</span><span class="params">()</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">     ...</span><br><span class="line">     obj = eval.evaluate(rowObject, version); <span class="comment">//(eval is ExprNodeColumnEvaluator)</span></span><br><span class="line">     ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator</span><br><span class="line"> <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> Object <span class="title">_evaluate</span><span class="params">(Object row, <span class="keyword">int</span> version)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> inspector.getStructFieldData(row, field); <span class="comment">//(inspector is LazySimpleStructObjectInspector)</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h1><ul><li>Processing<ul><li>During intilization<ul><li>hive creates SelectOperator, SelectOperator contains ExprNodeGenericFuncEvaluator, ExprNodeGenericFuncEvaluator contains GenericUDFAbs and DeferredExprObject (DeferredObject), DeferredExprObject contains ExprNodeColumnEvaluator (column[c1]).</li></ul></li><li>During executing<ul><li>SelectOperator processes row which is LazyStruct, and at last passed to LazySimpleStructObjectInspector (getStructFieldData) to get the actual data from binary data.</li></ul></li></ul></li><li>lazy-evaluation and short-circuiting<ul><li>We can notice that the value of the attribute which is involved in the UDF calculation is only evaluated just before use. The value is analyzed from binary format. This is very efficient.</li></ul></li></ul><h1 id="Cache-approach-in-GenericUDF"><a href="#Cache-approach-in-GenericUDF" class="headerlink" title="Cache approach in GenericUDF"></a>Cache approach in GenericUDF</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">    Object obj = arguments[<span class="number">0</span>].get();</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The obj got DeferredObject in GenericUDF evaluate function is  an LazyInteger object, it is always the same object even if evaluate function is invoked for processing a different row.</p><p>We should be careful when making use of cache to speedup GenericUDF calculation. Buffer the input and output, and returns buffered output if historical input comes. Since we always get the same object from the DeferredObject, if we use <code>equals</code> function to compare, it will always be true. We need to deep copy the inputs for further comparison. </p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>create git hooks in mac</title>
      <link href="/2018/12/15/create_git_hooks_in_mac/"/>
      <url>/2018/12/15/create_git_hooks_in_mac/</url>
      
        <content type="html"><![CDATA[<p>This article explains how to create git hooks in mac, and how to use the customized git hook chains.</p><h2 id="Preparation-for-creating-hook-chain"><a href="#Preparation-for-creating-hook-chain" class="headerlink" title="Preparation for creating hook chain"></a>Preparation for creating hook chain</h2><p>(This part refers to the <a href="https://stackoverflow.com/questions/8730514/chaining-git-hooks/8734391#8734391" target="_blank" rel="noopener">hook-chain</a>, and make some modification for running in mac.)<br></p><p>Motivation is that when you run git commit, the pre-commit script will be invoked before commit operation actually happen. If you want to do multiple checks in the pre-commit phrase, instead of putting all logic in one file of pre-commit, a better idea is separating the check logic in separated files, and trigger each check one by one as a chain.</p><ul><li><p>Create git templates hooks folder</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/.git-templates/hooks</span><br><span class="line"><span class="built_in">cd</span> ~/.git-templates/hooks</span><br></pre></td></tr></table></figure><p>All files in this folder will be copied to each git project’s hook folder when running git init. </p></li><li><p>Create hook-chain file under the hooks folder</p><p>file name : hook-chain</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">hookname=`basename <span class="variable">$0</span>`</span><br><span class="line">git_folder=`dirname <span class="variable">$0</span>`</span><br><span class="line"><span class="keyword">for</span> hook <span class="keyword">in</span> $(ls <span class="variable">$git_folder</span>/<span class="variable">$hookname</span>.*)</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="keyword">if</span> [ -f <span class="variable">$hook</span> ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"executing hook : <span class="variable">$hook</span>"</span></span><br><span class="line">        sh <span class="string">"<span class="variable">$hook</span>"</span></span><br><span class="line">        status=$?</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">test</span> <span class="variable">$status</span> -ne 0; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">"Hook <span class="variable">$hook</span> failed with error code <span class="variable">$status</span>"</span></span><br><span class="line">            <span class="built_in">exit</span> <span class="variable">$status</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>What this do is executing all hook files whose name begin with the current git hook. For example, when invoking git hook of pre-commit, this will invoke all scripts starting with pre-commit, such as pre-commit.json, pre-commit.yaml, etc.</p></li><li><p>Create hook files</p><p>Refer to <a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks" target="_blank" rel="noopener">git hooks</a> for all client side git hooks and their meaning. For example, pre-rebase, pre-commit, post-checkout, post-merge, pre-push, etc. Create the ones that you need under ~/.git-templates/hooks.</p><p>In my case, I want to create pre-commit hook, so I run:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s hook-chain pre-commit</span><br></pre></td></tr></table></figure><p>Then I will also create pre-commit.json, pre-commit.yaml, pre-commit.awskey in the same folder as explained in the following sessions. When pre-commit is invoked, it will trigger all these three files of pre-commit.json, pre-commit.yaml and pre-commit.awskey one by one.</p><p>Note that each hook file should be executable.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x ~/.git-templates/hooks/.</span><br></pre></td></tr></table></figure></li></ul><h2 id="Write-customized-hooks"><a href="#Write-customized-hooks" class="headerlink" title="Write customized hooks"></a>Write customized hooks</h2><h3 id="Prevent-commit-the-aws-access-key-and-secret-key"><a href="#Prevent-commit-the-aws-access-key-and-secret-key" class="headerlink" title="Prevent commit the aws access key and secret key"></a>Prevent commit the aws access key and secret key</h3><p>(script is <a href="https://gist.github.com/saliceti/7eb0ba0bb5ed875df515" target="_blank" rel="noopener">from</a>)</p><ul><li>file name : pre-commit.awskey<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install globally using https://coderwall.com/p/jp7d5q/create-a-global-git-commit-hook</span></span><br><span class="line"><span class="comment"># The checks are simple and can give false positives. Amend the hook in the specific repository.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> git rev-parse --verify HEAD &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    against=HEAD</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="comment"># Initial commit: diff against an empty tree object</span></span><br><span class="line">    EMPTY_TREE=$(git <span class="built_in">hash</span>-object -t tree /dev/null)</span><br><span class="line">    against=<span class="variable">$EMPTY_TREE</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Redirect output to stderr.</span></span><br><span class="line"><span class="built_in">exec</span> 1&gt;&amp;2</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Check changed files for an AWS keys</span></span><br><span class="line">FILES=$(git diff --cached --name-only <span class="variable">$against</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$FILES</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">    KEY_ID=$(grep -rE --line-number <span class="string">'(^|[^A-Za-z0-9/+=])AKIA[A-Z0-9]&#123;16&#125;($|[^A-Za-z0-9/+=])'</span> <span class="variable">$FILES</span>)</span><br><span class="line">    KEY=$(grep -rE --line-number <span class="string">'(^|[^A-Za-z0-9/+=])[A-Za-z0-9/+=]&#123;40&#125;($|[^A-Za-z0-9/+=])'</span> <span class="variable">$FILES</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$KEY_ID</span>"</span> ] || [ -n <span class="string">"<span class="variable">$KEY</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">exec</span> &lt; /dev/tty <span class="comment"># Capture input</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"=========== Possible AWS Access Key IDs ==========="</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;KEY_ID&#125;</span>"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"=========== Possible AWS Secret Access Keys ==========="</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;KEY&#125;</span>"</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">read</span> -p <span class="string">"[AWS Key Check] Possible AWS keys found. Commit files anyway? (y/N) "</span> yn</span><br><span class="line">            <span class="keyword">if</span> [ <span class="string">"<span class="variable">$yn</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">                yn=<span class="string">'N'</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">            <span class="keyword">case</span> <span class="variable">$yn</span> <span class="keyword">in</span></span><br><span class="line">                [Yy] ) <span class="built_in">exit</span> 0;;</span><br><span class="line">                [Nn] ) <span class="built_in">exit</span> 1;;</span><br><span class="line">                * ) <span class="built_in">echo</span> <span class="string">"Please answer y or n for yes or no."</span>;;</span><br><span class="line">            <span class="keyword">esac</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">        <span class="built_in">exec</span> &lt;&amp;- <span class="comment"># Release input</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Normal exit</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></li></ul><h3 id="Prevent-commiting-invalid-json-files"><a href="#Prevent-commiting-invalid-json-files" class="headerlink" title="Prevent commiting invalid json files"></a>Prevent commiting invalid json files</h3><ul><li>file name : pre-commit.json<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">git_dir=$(git rev-parse --show-toplevel)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> $(git diff-index --name-only --diff-filter=ACM --cached HEAD -- \</span><br><span class="line">    | grep -E <span class="string">'\.((js)|(json))$'</span>); <span class="keyword">do</span></span><br><span class="line">    python -mjson.tool <span class="variable">$file</span> 2&gt; /dev/null</span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ] ; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">"Find unbroken json in <span class="variable">$git_dir</span>/<span class="variable">$file</span>, is that what you intended? [y|n] "</span> -n 1 -r &lt; /dev/tty</span><br><span class="line">        <span class="built_in">echo</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$REPLY</span> | grep -E <span class="string">'^[Yy]$'</span> &gt; /dev/null</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">exit</span> 0</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="Prevent-commiting-yaml-files-with-quote-in-key"><a href="#Prevent-commiting-yaml-files-with-quote-in-key" class="headerlink" title="Prevent commiting yaml files with quote in key"></a>Prevent commiting yaml files with quote in key</h3><p>Note that standard yaml allows containing quote in keys, but that is probably not something we intented to do.</p><ul><li><p>file name : pre-commit.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">git_dir=$(git rev-parse --show-toplevel)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> $(git diff-index --name-only --diff-filter=ACM --cached HEAD -- \</span><br><span class="line">    | grep -E <span class="string">'\.((yaml)|(yml))$'</span>); <span class="keyword">do</span></span><br><span class="line">    python3 <span class="string">"<span class="variable">$&#123;git_dir&#125;</span>/.git/hooks/is_yaml_key_contains_quote.py"</span> <span class="variable">$file</span></span><br><span class="line">    <span class="keyword">if</span> [ $? -ne 0 ] ; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">read</span> -p <span class="string">"Find quote in key in <span class="variable">$git_dir</span>/<span class="variable">$file</span>, is that what you intended? [y|n] "</span> -n 1 -r &lt; /dev/tty</span><br><span class="line">        <span class="built_in">echo</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$REPLY</span> | grep -E <span class="string">'^[Yy]$'</span> &gt; /dev/null</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">exit</span> 0</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></li><li><p>file name : is_yaml_key_contains_quote.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_quote_in_key</span><span class="params">(json_obj)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> json_obj:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> type(json_obj) <span class="keyword">is</span> list:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_obj:</span><br><span class="line">            find_quote_in_key(item)</span><br><span class="line">    <span class="keyword">if</span> type(json_obj) <span class="keyword">is</span> dict:</span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> json_obj.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"\""</span> <span class="keyword">in</span> key <span class="keyword">or</span> <span class="string">"'"</span> <span class="keyword">in</span> key:</span><br><span class="line">                print(<span class="string">"quote is in : &#123;&#125;"</span>.format(key))</span><br><span class="line">                <span class="keyword">global</span> is_quote_found</span><br><span class="line">                is_quote_found = <span class="keyword">True</span></span><br><span class="line">            find_quote_in_key(value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">file_path = sys.argv[<span class="number">1</span>]</span><br><span class="line">is_quote_found = <span class="keyword">False</span></span><br><span class="line">yaml_content = <span class="string">""</span></span><br><span class="line"><span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> stream:</span><br><span class="line">    yaml_content = yaml.load(stream)</span><br><span class="line">json_obj = json.dumps(yaml_content)</span><br><span class="line">find_quote_in_key(json.loads(json_obj))</span><br><span class="line"><span class="keyword">if</span> is_quote_found:</span><br><span class="line">    exit(<span class="number">1</span>)</span><br><span class="line">exit(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Note : It is needed to install python3 and pyymal for running this python hook.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew install python3</span><br><span class="line">pip3 install pyyaml</span><br></pre></td></tr></table></figure></li></ul><h3 id="Preventing-push-to-the-master-branch"><a href="#Preventing-push-to-the-master-branch" class="headerlink" title="Preventing push to the master branch"></a>Preventing push to the master branch</h3><p>(Script is <a href="https://github.com/smiley/git-hooks/blob/master/pre-push/prevent-push-to-protected-branch.sh" target="_blank" rel="noopener">from</a>)</p><ul><li>file name : pre-push.protect-master<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">protected_branch=<span class="string">'push-test'</span></span><br><span class="line">current_branch=$(git symbolic-ref HEAD | sed -e <span class="string">'s,.*/\(.*\),\1,'</span>)</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$protected_branch</span> = <span class="variable">$current_branch</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">read</span> -p <span class="string">"You're about to push <span class="variable">$protected_branch</span>, is that what you intended? [y|n] "</span> -n 1 -r &lt; /dev/tty</span><br><span class="line">    <span class="built_in">echo</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">echo</span> <span class="variable">$REPLY</span> | grep -E <span class="string">'^[Yy]$'</span> &gt; /dev/null</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></li></ul><p>Don’t forget to execute : ln -s hook-chain pre-push.</p><h2 id="Install-hooks"><a href="#Install-hooks" class="headerlink" title="Install hooks"></a>Install hooks</h2><p>In any git project, run git init will install the hooks under .git/hooks/ folder, however, the existing files won’t be overriden, meaning it is needed to delete the existing hooks in the git repository after updating the hook scripts.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm .git/hooks/*</span><br><span class="line">git init</span><br></pre></td></tr></table></figure><p>I suggest creating an alias command for doing that.</p><p>Copy the following in the ~/.profile file</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> my_git_init=<span class="string">'rm .git/hooks/*; git init'</span></span><br></pre></td></tr></table></figure><p>Then run my_git_init under the git project will always delete the existing hooks and install the latest ones.</p>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop local mode and distributed mode</title>
      <link href="/2018/12/09/hadoop-local-and-distributed-mode/"/>
      <url>/2018/12/09/hadoop-local-and-distributed-mode/</url>
      
        <content type="html"><![CDATA[<p>Whether a job runs in local mode or distributed mode is decided by <em>mapreduce.framework.name</em>. In local mode, the mapper and reducer will run locally in the same JVM with the client. In distributed mode, the job will be submitted to the resource manager. This article aims at digging into the related source code.</p><h2 id="Flow-of-execution"><a href="#Flow-of-execution" class="headerlink" title="Flow of execution"></a>Flow of execution</h2><ul><li><p>Create JobClient</p><ul><li>Create <em>JobClient</em> which is primary interact with the cluster.</li><li><p>During JobClient initialization, create <em>Cluster</em> which provides access to the cluster.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.JobClient</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(JobConf conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">       ...    </span><br><span class="line">       cluster = <span class="keyword">new</span> Cluster(conf);</span><br><span class="line">       ...  </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>During Cluster initialization, use loaded ClientProtocalProvider list to create ClientProtocol until get one, ClientProtocal is the protocal for client and central jobtracker to communicate.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapreduce.Cluster</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">()</span> </span>&#123;</span><br><span class="line">         ...</span><br><span class="line">    <span class="keyword">for</span> (ClientProtocolProvider provider : providerList) &#123;</span><br><span class="line">        ClientProtocol clientProtocol = <span class="keyword">null</span>;</span><br><span class="line">        clientProtocol = provider.create(..., conf);</span><br><span class="line">        <span class="keyword">if</span> (clientProtocol != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>There are two kinds of ClientProtocalProvider, one is LocalClientProtocolProvider for local, the other is YarnClientProtocolProvider for communicating with yarn.</p><ul><li><p>If <em>mapreduce.framework.name</em> is yarn, then create YarnRunner.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.YarnClientProtocolProvider</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ClientProtocol <span class="title">create</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (MRConfig.YARN_FRAMEWORK_NAME.equals(conf.get(MRConfig.FRAMEWORK_NAME))) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> YARNRunner(conf);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li><li><p>If <em>mapreduce.framework.name</em> is local, then create LocalJobRunner, and set map to be 1.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalClientProtocolProvider</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ClientProtocol <span class="title">create</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    String framework =</span><br><span class="line">        conf.get(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);</span><br><span class="line">    <span class="keyword">if</span> (!MRConfig.LOCAL_FRAMEWORK_NAME.equals(framework)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    conf.setInt(JobContext.NUM_MAPS, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> LocalJobRunner(conf);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>Submit job through JobClient</p><ul><li><p>Get new JobId</p><ul><li><p>For local execution, generate local new jobId using LocalJobRunner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalJobRunner</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">synchronized</span> org.apache.hadoop.mapreduce.<span class="function">JobID <span class="title">getNewJobID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> org.apache.hadoop.mapreduce.JobID(<span class="string">"local"</span> + randid, ++jobid);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>For distributed execution, get new JobId from resource manager through YarnRunner</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.YARNRunner</span><br><span class="line">  <span class="function"><span class="keyword">public</span> JobID <span class="title">getNewJobID</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> resMgrDelegate.getNewJobID();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Actual submit job</p><ul><li><p>For local execution</p><ul><li><p>Create a Job object. Job extends Thread, and can be run as a Thread.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalJobRunner</span><br><span class="line">   <span class="keyword">public</span> org.apache.hadoop.mapreduce.<span class="function">JobStatus <span class="title">submitJob</span><span class="params">(...)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">      Job job = <span class="keyword">new</span> Job(JobID.downgrade(jobid), jobSubmitDir);</span><br><span class="line">      ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure></li><li><p>At the end of the constructor of Job, it starts itself as a Thread.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.LocalJobRunner</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Job</span><span class="params">(JobID jobid, String jobSubmitDir)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">this</span>.start();</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>For distributed execution, submit the job to resource manager.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.YARNRunner</span><br><span class="line">    public JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)&#123;</span><br><span class="line">        ...</span><br><span class="line">        ApplicationId applicationId =</span><br><span class="line">          resMgrDelegate.submitApplication(appContext);</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>We can see that, whether a job runs in local mode or distributed mode is decided by <em>mapreduce.framework.name</em>. In local mode, the mapper and reducer will run locally in the same JVM with the client. In distributed mode, the job will be submitted to the resource manager.</p>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive configuration understanding</title>
      <link href="/2018/11/19/hive-configuration-understanding/"/>
      <url>/2018/11/19/hive-configuration-understanding/</url>
      
        <content type="html"><![CDATA[<p>This article aims at introducing what are the manually configured settings that override the default during using hive.</p><h2 id="environment"><a href="#environment" class="headerlink" title="environment"></a>environment</h2><p>This article is based on hive 2.3, hadoop 2.7, running hive on mapreduce.</p><h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>Hive and hadoop have many configurations and it is not striaightforward to know what configurations need to be override manually. This article aims at listing up all such configurations that we can special take care of.</p><h2 id="configuration-reference"><a href="#configuration-reference" class="headerlink" title="configuration reference"></a>configuration reference</h2><p>There are the links of the latest explanations of the configurations in the official wiki . </p><ul><li>hadoop<ul><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml" target="_blank" rel="noopener">core-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">hdfs-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="noopener">mapred-default.xml</a></li><li><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a></li></ul></li><li>hive<ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties" target="_blank" rel="noopener">hive-site.xml</a></li></ul></li></ul><h2 id="overridden-configurations"><a href="#overridden-configurations" class="headerlink" title="overridden configurations"></a>overridden configurations</h2><p>These are the configurations that need to take special care and be configured manually to override the default value.</p><ul><li><p>Basic settings</p><ul><li>purpose<ul><li>There are some basic settings.</li></ul></li><li>configuration<ul><li>fs.defaultFS<ul><li>default : file:///</li><li>configured :  hdfs://&lt;namenode_ip&gt;:8020</li><li>explain : This configures the default file system. By default, it is local file system, change it to use distributed file system.</li></ul></li><li>mapreduce.framework.name<ul><li>default : local</li><li>configured : yarn</li><li>explain : The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.</li></ul></li><li>yarn.resourcemanager.hostname <ul><li>default:  0.0.0.0</li><li>configured: <the real="" ip=""></the></li><li>explain: The hostname of the RM.</li></ul></li></ul></li></ul></li><li><p>Enable yarn log aggregation</p><ul><li>purpose <ul><li>By default, yanr log aggregation is disabled, and yarn logs(container logs) are saved in the local of the instances where the containers run. In order to check the log, it is needed to first check the instance ip of a container, then login the instance and open the corresponding log file. When yarn log aggregation is enabled, the container’s log will be aggregated to some configured place(HDFS, s3, etc.), and it is able to check the yarn log through the yarn UI, which becomes much simpler.</li></ul></li><li>configuration <ul><li>yarn.log-aggregation-enable<ul><li>default : false</li><li>configured : true</li><li>explain : This enables the yarn log aggregation.</li></ul></li><li>yarn.nodemanager.remote-app-log-dir<ul><li>default : /tmp/logs for hdfs</li><li>configured : e.g. s3a://… for s3</li><li>explain : This specifies the prefix of the actual path of the aggregated logs. You may want to include the cluster name in the path.</li></ul></li><li>yarn.log-aggregation.retain-seconds<ul><li>default : -1</li><li>explain : This specifies how long the aggregated logs will be kept. The default value -1 indicates keeping the logs forever. Too big value may cause a waste of disk resource, too small may make you cannot find logs when investigate recent issues. 2 weeks or so may be helpful.</li></ul></li><li>yarn.nodemanager.log-aggregation.compression-type<ul><li>default: none</li><li>configured: gz</li><li>explain: If, and how, aggregated logs should be compressed.</li></ul></li><li>mapreduce.job.userlog.retain.hours<ul><li>default : 24</li><li>explain : This specifies the maximum time in hours that the local yarn logs will be retained after job completion. You may want to adjust this setting together.</li></ul></li><li>mapreduce.jobhistory.max-age-ms<ul><li>default : 604800000</li><li>explain : Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week). We can make it 2 weeks.</li></ul></li></ul></li><li><a href="https://renenyffenegger.ch/notes/development/Apache/Hadoop/Ycccc-=ARN/log-aggregation" target="_blank" rel="noopener">helpful links</a></li></ul></li><li>Clean HDFS trash regularly<ul><li>purpose<ul><li>By default, the trash folder is not cleaned up automatically. We can configure to make it be cleaned up regularly to reduce disk cost.</li></ul></li><li>configuration<ul><li>fs.trash.interval    <ul><li>default : 0</li><li>configured : 360</li><li>explain : will be deleted after 6 hours</li></ul></li></ul></li></ul></li><li>Enable hive client authorization<ul><li>purpose<ul><li>By default, hive client authorization is disabled. Enabling hive client authorization can help prevent users from doing operations they are not supposed to do.</li></ul></li><li>configuration<ul><li>hive.security.authorization.enabled<ul><li>default : false</li><li>configured : true</li><li>explain : Enables the hive client authorization.</li></ul></li><li>There are also other settings as needed. I will leave it as TODO.</li></ul></li><li>helpful links<ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization" target="_blank" rel="noopener">SQL Standard Based Hive Authorization</a></li></ul></li></ul></li><li>Enable HDFS compression<ul><li>configuration<ul><li>io.compression.codecs<ul><li>default : </li><li>configured :  org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec</li><li>explain : A comma-separated list of the compression codec classes that can be used for compression/decompression which precedence of others loaded by from the classpath.</li></ul></li><li>mapreduce.map.output.compress<ul><li>default : false</li><li>configured : true</li><li>explain : Compress the output of map before sending accross the network.</li></ul></li><li>mapreduce.map.output.compress.codec<ul><li>default : org.apache.hadoop.io.compress.DefaultCodec</li><li>configured : org.apache.hadoop.io.compress.SnappyCodec or org.apache.hadoop.io.compress.GzipCodec</li><li>explain : If the map outputs are compressed, how should they be compressed?</li></ul></li><li>mapreduce.output.fileoutputformat.compress.type<ul><li>default : RECORD</li><li>configured : BLOCK</li><li>explain : If the job outputs are to compressed as SequenceFiles, how should they be compressed?</li></ul></li></ul></li></ul></li><li>Fail jobs when exceed disk usage<ul><li>configuration<ul><li>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<ul><li>default : true</li><li>configured : false</li></ul></li></ul></li></ul></li><li>dfs related<ul><li>configuration<ul><li>dfs.namenode.avoid.read.stale.datanode<ul><li>default : false</li><li>configured : true</li></ul></li><li>dfs.namenode.avoid.write.stale.datanode<ul><li>default : false</li><li>configured : true</li></ul></li></ul></li></ul></li><li>save job result in s3(option)<ul><li>configuration<ul><li>mapreduce.jobhistory.done-dir <ul><li>default: ${yarn.app.mapreduce.am.staging-dir}/history/done </li><li>configured: s3 path</li></ul></li></ul></li></ul></li><li>speculative configs<ul><li>configuration<ul><li>mapreduce.map.speculative <ul><li>default: true</li><li>configured: false</li></ul></li><li>mapreduce.reduce.speculative<ul><li>default: true</li><li>configured: false</li></ul></li></ul></li></ul></li><li>enable namenode restart<ul><li>configuration<ul><li>yarn.nodemanager.recovery.enabled<ul><li>default: false</li><li>configured: true</li></ul></li><li>yarn.nodemanager.address<ul><li>default:  ${yarn.nodemanager.hostname}:0</li><li>configured: 0.0.0.0:45454</li><li>explain: Default value makes namenode use different port before and after a restart, and the running clients will be broken after the restart, explicitly setting the port can solve the issue.</li></ul></li></ul></li><li><a href="https://hadoop.apache.org/docs/r2.8.0/hadoop-yarn/hadoop-yarn-site/NodeManager.html" target="_blank" rel="noopener">helpful links</a></li></ul></li><li>enable resource manager restart<ul><li>configuration<ul><li>yarn.resourcemanager.recovery.enabled<ul><li>default: false</li><li>configured: true</li></ul></li></ul></li><li><a href="https://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank" rel="noopener">helpful link</a></li></ul></li><li>hive configs<ul><li>configuration<ul><li>hive.auto.convert.join<ul><li>configured: false</li></ul></li><li>hive.auto.convert.join.noconditionaltask<ul><li>configured: false</li></ul></li><li>hive.exec.compress.intermediate<ul><li>configured: true</li></ul></li><li>hive.exec.parallel<ul><li>configured: true</li></ul></li><li>hive.fetch.task.conversion<ul><li>default: more</li><li>configured: none</li></ul></li><li>hive.groupby.orderby.position.alias<ul><li>configured: true</li></ul></li><li>hive.log.explain.output<ul><li>configured: true</li></ul></li><li>hive.mapred.reduce.tasks.speculative.execution<ul><li>configured: false</li></ul></li><li>hive.optimize.reducededuplication<ul><li>configured: false</li></ul></li><li>hive.resultset.use.unique.column.names<ul><li>configured: false</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive scratch directory</title>
      <link href="/2018/11/17/hive-scratch-working-directory/"/>
      <url>/2018/11/17/hive-scratch-working-directory/</url>
      
        <content type="html"><![CDATA[<p>This article aims at explaining hive scratch directory.</p><h2 id="Scratch-directory-usage"><a href="#Scratch-directory-usage" class="headerlink" title="Scratch directory usage"></a>Scratch directory usage</h2><p>Hive scratch directory is a temporary working space for storing the plans for different map/reduce stages of the query as well as the intermediate outputs of these stages.</p><h2 id="Scratch-directory-clean-up"><a href="#Scratch-directory-clean-up" class="headerlink" title="Scratch directory clean up"></a>Scratch directory clean up</h2><p>Hive scratch directory is usually cleaned up by the hive client when the query finishes. However, some data may be left behind if hive client terminates abnormally. Hive server2 contains a thread (<a href="https://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/session/ClearDanglingScratchDir.java#L43-L55" target="_blank" rel="noopener">ClearDanglingScratchDir</a>) to clean up the remaining files, we can also write our own script to do the clean up if not running Hive server2.</p><h2 id="Scratch-directory-types"><a href="#Scratch-directory-types" class="headerlink" title="Scratch directory types"></a>Scratch directory types</h2><p>Hive queries may be procesed in local(the instance which hive client is invoked) or in remote(hadoop cluster). There also have two kinds of scratch dir accordingly, one in local, the other in hdfs.</p><h2 id="Scratch-directory-configuration"><a href="#Scratch-directory-configuration" class="headerlink" title="Scratch directory configuration"></a>Scratch directory configuration</h2><p><em>hive.exec.local.scratchdir</em> for local and <em>hive.exec.scratchdir</em> for HDFS(<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">hive configuration</a>).</p><p>Note that since hive 0.14.0, the HDFS scratch directory created will be <em>${hive.exec.scratchdir}\${user_name}</em> indicating it supports multi-tenant natively and there is no need to include user_id in the value.</p><h2 id="Scratch-directory-example"><a href="#Scratch-directory-example" class="headerlink" title="Scratch directory example"></a>Scratch directory example</h2><p>We run a simple query and see what are the files generated in the scratch directory.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--@INTERNAL hive_version:hive2</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> col1&gt;<span class="number">0</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> col2</span><br></pre></td></tr></table></figure></p><ul><li><p>when query is submitted to the cluster and waiting for containers</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br></pre></td></tr></table></figure><ul><li>-ext- : a dir indicates the final query output</li><li>-mr- : a output directory for each MapReduce job</li><li>map.xml : map plan</li><li>reduce.xml : reduce plan</li></ul></li><li><p>when query is running in the hadoop cluster</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001/000000_0</span><br></pre></td></tr></table></figure><ul><li>data is generated in the -ext- dir  </li></ul></li><li>when query finished, scratch dir with all files are cleaned up</li></ul><h2 id="Scratch-directory-related-INFO-logs"><a href="#Scratch-directory-related-INFO-logs" class="headerlink" title="Scratch directory related INFO logs"></a>Scratch directory related INFO logs</h2><p>These info logs are generated when running the query in the previous section.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/_tmp_space.db</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020$/&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br><span class="line">exec.FileSinkOperator: Moving tmp dir: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001 to: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001</span><br></pre></td></tr></table></figure><ul><li><p>First several HDFS scratch directories are created during start <a href="(https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L698-L704">SessionState</a>.</p></li><li><p>_hive.hdfs.session.path = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}</p></li><li>hive.exec.plan = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}/${context execution id}-${task runner id}/-mr-${path id}/${random uuid}<ul><li><a href="https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L414" target="_blank" rel="noopener">hive.session.id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L313" target="_blank" rel="noopener">context execution id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L491" target="_blank" rel="noopener">task running id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L685" target="_blank" rel="noopener">path id</a></li><li><a href="https://github.com/apache/hive/blob/6d713b6564ecb9d1ae0db66c3742d2a8bc347211/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L668" target="_blank" rel="noopener">random uuid</a></li></ul></li><li>map.xml path = ${hive.exec.plan}/map.xml</li><li>reduce.xml path = ${hive.exec.plan}/reduce.xml</li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>How to fast fail hive jobs</title>
      <link href="/2018/11/15/hive-job-fast-fail/"/>
      <url>/2018/11/15/hive-job-fast-fail/</url>
      
        <content type="html"><![CDATA[<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>When running hive jobs in hadoop clusters on mapreduce, we always set the limitation of how much local and hdfs disk a job can use at most. Such limitation is per job basis and it can prevent a single job from using up too much disk resource and causing a node or a cluster unstable.</p><p>During running one job, if any task belong to the job finds the limitation is exceeded, the task will fail and at such time, we want to <strong>fast fail the job instead of retrying the failed tasks many times</strong>. Because since the local or hdfs max disk usage limitation is reached, the retry will probably still fail and is not much helpful.</p><h2 id="appoaches"><a href="#appoaches" class="headerlink" title="appoaches"></a>appoaches</h2><p>I will introduce how to configure to fast fail hive jobs when the local or hdfs limitation is exceeded.</p><h3 id="fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node"><a href="#fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node" class="headerlink" title="fast fail a job when too much local disk is used in one node"></a>fast fail a job when too much <em>local disk</em> is used in one node</h3><ul><li><p><strong>hadoop configuration</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.bytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>322122547200<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>implementation detail</strong><br>A task(org.apache.hadoop.mapred.Task) contains a thread checking all local working directories(mapreduce.cluster.local.dir), and will fast fail the job if any local working directory exceeds the limitation.</p></li><li><p><strong>required hadoop version</strong><br>3.1.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7022" target="_blank" rel="noopener">MAPREDUCE-7022</a></p></li></ul><h3 id="fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster"><a href="#fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster" class="headerlink" title="fast fail a job when too much hdfs disk is used in the cluster"></a>fast fail a job when too much <em>hdfs disk</em> is used in the cluster</h3><ul><li><p><strong>configuration</strong></p><ul><li><strong>hive configuration</strong><br>Set hive config hive.exec.scratchdir to per job basis, and set the quota limitation for the path.</li><li><strong>hadoop configuration</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>internal implementation</strong><br>A subclass of ClusterStorageCapacityExceededException will be thrown if the cluster capacity limitation is exceeded and then YarnChild will fast fail the job.</p></li><li><p><strong>required hadoop version</strong><br>3.3.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7148" target="_blank" rel="noopener">MAPREDUCE-7148</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
