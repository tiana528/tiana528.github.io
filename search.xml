<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>hive scratch directory</title>
      <link href="/2018/11/17/hive-scratch-working-directory/"/>
      <url>/2018/11/17/hive-scratch-working-directory/</url>
      
        <content type="html"><![CDATA[<p>This article aims at explaining hive scratch directory.</p><h2 id="Scratch-directory-usage"><a href="#Scratch-directory-usage" class="headerlink" title="Scratch directory usage"></a>Scratch directory usage</h2><p>Hive scratch directory is a temporary working space for storing the plans for different map/reduce stages of the query as well as the intermediate outputs of these stages.</p><h2 id="Scratch-directory-clean-up"><a href="#Scratch-directory-clean-up" class="headerlink" title="Scratch directory clean up"></a>Scratch directory clean up</h2><p>Hive scratch directory is usually cleaned up by the hive client when the query finishes. However, some data may be left behind if hive client terminates abnormally. Hive server2 contains a thread (<a href="https://github.com/apache/hive/blob/f37c5de6c32b9395d1b34fa3c02ed06d1bfbf6eb/ql/src/java/org/apache/hadoop/hive/ql/session/ClearDanglingScratchDir.java#L43-L55" target="_blank" rel="noopener">ClearDanglingScratchDir</a>) to clean up the remaining files, we can also write our own script to do the clean up if not running Hive server2.</p><h2 id="Scratch-directory-types"><a href="#Scratch-directory-types" class="headerlink" title="Scratch directory types"></a>Scratch directory types</h2><p>Hive queries may be procesed in local(the instance which hive client is invoked) or in remote(hadoop cluster). There also have two kinds of scratch dir accordingly, one in local, the other in hdfs.</p><h2 id="Scratch-directory-configuration"><a href="#Scratch-directory-configuration" class="headerlink" title="Scratch directory configuration"></a>Scratch directory configuration</h2><p><em>hive.exec.local.scratchdir</em> for local and <em>hive.exec.scratchdir</em> for HDFS(<a href="https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties" target="_blank" rel="noopener">hive configuration</a>).</p><p>Note that since hive 0.14.0, the HDFS scratch directory created will be <em>${hive.exec.scratchdir}\${user_name}</em> indicating it supports multi-tenant natively and there is no need to include user_id in the value.</p><h2 id="Scratch-directory-example"><a href="#Scratch-directory-example" class="headerlink" title="Scratch directory example"></a>Scratch directory example</h2><p>We run a simple query and see what are the files generated in the scratch directory.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--@INTERNAL hive_version:hive2</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> tb1 </span><br><span class="line"><span class="keyword">where</span> col1&gt;<span class="number">0</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> col2</span><br></pre></td></tr></table></figure></p><ul><li><p>when query is submitted to the cluster and waiting for containers</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br></pre></td></tr></table></figure><ul><li>-ext- : a dir indicates the final query output</li><li>-mr- : a output directory for each MapReduce job</li><li>map.xml : map plan</li><li>reduce.xml : reduce plan</li></ul></li><li><p>when query is running in the hadoop cluster</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- $&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001/000000_0</span><br></pre></td></tr></table></figure><ul><li>data is generated in the -ext- dir  </li></ul></li><li>when query finished, scratch dir with all files are cleaned up</li></ul><h2 id="Scratch-directory-related-INFO-logs"><a href="#Scratch-directory-related-INFO-logs" class="headerlink" title="Scratch directory related INFO logs"></a>Scratch directory related INFO logs</h2><p>These info logs are generated when running the query in the previous section.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4</span><br><span class="line">session.SessionState: Created HDFS directory: /$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/_tmp_space.db</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020$/&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">common.FileUtils: Creating directory if it doesn&apos;t exist: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1</span><br><span class="line">ql.Context: New scratch dir is hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/map.xml</span><br><span class="line">exec.Utilities: PLAN PATH = hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-2/-mr-10004/8cf23c5d-9a81-4e99-ae69-d8b99eee1a08/reduce.xml</span><br><span class="line">exec.FileSinkOperator: Moving tmp dir: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/_tmp.-ext-10001 to: hdfs://$&#123;namenode_ip&#125;:8020/$&#123;hive.exec.scratchdir&#125;/$&#123;job_id&#125;/$&#123;user_name&#125;/bf17195d-b591-457a-a5e1-28426156c7f4/hive_2018-11-18_11-32-47_530_1102432233025308705-1/-mr-10000/.hive-staging_hive_2018-11-18_11-32-47_530_1102432233025308705-1/-ext-10001</span><br></pre></td></tr></table></figure><ul><li><p>First several HDFS scratch directories are created during start <a href="(https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L698-L704">SessionState</a>.</p></li><li><p>_hive.hdfs.session.path = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}</p></li><li>hive.exec.plan = ${hive.exec.scratchdir}/${job_id}/${user_name}/${hive.session.id}/${context execution id}-${task runner id}/-mr-${path id}/${random uuid}<ul><li><a href="https://github.com/apache/hive/blob/840dd431f3772772fc57060e27e3f2bee72a8936/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java#L414" target="_blank" rel="noopener">hive.session.id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L313" target="_blank" rel="noopener">context execution id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L491" target="_blank" rel="noopener">task running id</a></li><li><a href="https://github.com/apache/hive/blob/b4302bb7ad967f15ca1b708685b2ac669e3cf037/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L685" target="_blank" rel="noopener">path id</a></li><li><a href="https://github.com/apache/hive/blob/6d713b6564ecb9d1ae0db66c3742d2a8bc347211/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L668" target="_blank" rel="noopener">random uuid</a></li></ul></li><li>map.xml path = ${hive.exec.plan}/map.xml</li><li>reduce.xml path = ${hive.exec.plan}/reduce.xml</li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>How to fast fail hive jobs</title>
      <link href="/2018/11/15/hive-job-fast-fail/"/>
      <url>/2018/11/15/hive-job-fast-fail/</url>
      
        <content type="html"><![CDATA[<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><p>When running hive jobs in hadoop clusters on mapreduce, we always set the limitation of how much local and hdfs disk a job can use at most. Such limitation is per job basis and it can prevent a single job from using up too much disk resource and causing a node or a cluster unstable.</p><p>During running one job, if any task belong to the job finds the limitation is exceeded, the task will fail and at such time, we want to <strong>fast fail the job instead of retrying the failed tasks many times</strong>. Because since the local or hdfs max disk usage limitation is reached, the retry will probably still fail and is not much helpful.</p><h2 id="appoaches"><a href="#appoaches" class="headerlink" title="appoaches"></a>appoaches</h2><p>I will introduce how to configure to fast fail hive jobs when the local or hdfs limitation is exceeded.</p><h3 id="fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node"><a href="#fast-fail-a-job-when-too-much-local-disk-is-used-in-one-node" class="headerlink" title="fast fail a job when too much local disk is used in one node"></a>fast fail a job when too much <em>local disk</em> is used in one node</h3><ul><li><p><strong>hadoop configuration</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.bytes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>322122547200<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>implementation detail</strong><br>A task(org.apache.hadoop.mapred.Task) contains a thread checking all local working directories(mapreduce.cluster.local.dir), and will fast fail the job if any local working directory exceeds the limitation.</p></li><li><p><strong>required hadoop version</strong><br>3.1.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7022" target="_blank" rel="noopener">MAPREDUCE-7022</a></p></li></ul><h3 id="fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster"><a href="#fast-fail-a-job-when-too-much-hdfs-disk-is-used-in-the-cluster" class="headerlink" title="fast fail a job when too much hdfs disk is used in the cluster"></a>fast fail a job when too much <em>hdfs disk</em> is used in the cluster</h3><ul><li><p><strong>configuration</strong></p><ul><li><strong>hive configuration</strong><br>Set hive config hive.exec.scratchdir to per job basis, and set the quota limitation for the path.</li><li><strong>hadoop configuration</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.dfs.storage.capacity.kill-limit-exceed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>internal implementation</strong><br>A subclass of ClusterStorageCapacityExceededException will be thrown if the cluster capacity limitation is exceeded and then YarnChild will fast fail the job.</p></li><li><p><strong>required hadoop version</strong><br>3.3.0 or apply patch <a href="https://issues.apache.org/jira/browse/MAPREDUCE-7148" target="_blank" rel="noopener">MAPREDUCE-7148</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> big data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
